{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15688 Project - Lyrics Generator & Classificator\n",
    "\n",
    "“Music can change the word because it can change people.” said by the Legendary U2 rocker Bono. A beautiful song usually has memorable lyrics that sometimes change people. However, it is not an easy task to write good lyrics. \n",
    "\n",
    "The aim of our project is to create a lyric generation model based on existing lyrics of different music genres - pop, rock, hip hop, etc - using machine learning algorithms that are common in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention:** <br>\n",
    "We develop this project via python scripts instead of Jupyter Notebook. <br>\n",
    "This tutorial mainly walks you through each step.  <br>\n",
    "It is highly recommended to go over this project via the command line we suggest instead of on this notebook. (In case some unexpected error.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data Collection\n",
    "\n",
    "### Part 1. Singer and song collection\n",
    "In order to train the lyric model, the first step is to collect lyrics by genre. We collected the male and female artists' names from [music.163.com ](https://music.163.com/#/discover/artist/cat?id=2001)by copying the information on the webpage and saved them as csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting artists' names, we use the [musixmatch](http://api.musixmatch.com/ws/1.1/) api to collect the genre and name of songs of the artists. The results are exported as csv file so that we can count the most frequent genres among all the songs we collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "15688 final project - lyric generator\n",
    "\n",
    "data collection\n",
    "\n",
    "retrieve the artists, genres and tracks and export to csv file\n",
    "\n",
    "API used: musixmatch Developer\n",
    "documentation: https://developer.musixmatch.com/documentation\n",
    "\n",
    "'''\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# load api key\n",
    "with open(\"../musicmatch_api.key\",'r') as f:\n",
    "    api = f.read()\n",
    "\n",
    "root = \"http://api.musixmatch.com/ws/1.1/\"\n",
    "\n",
    "def get_artist(api, pageNum, page_size=100, country = \"us\"):\n",
    "\n",
    "    '''\n",
    "    getting top artists and their genres\n",
    "    Args:\n",
    "        api: API key\n",
    "        pageNum: the page number for paginated results\n",
    "        page_size: the page size for paginated results. Range is 1 to 100\n",
    "        country: country of the artist ranking\n",
    "    Return:\n",
    "        df: a pandas dataframe containing artists, genres and genre id\n",
    "        all_genres: a set of all genres related to the artists found\n",
    "    '''\n",
    "    result = []\n",
    "    all_genres = set()\n",
    "    for i in range(pageNum):\n",
    "        param = {\n",
    "            \"apikey\":api,\n",
    "            \"country\": \"country\",\n",
    "            \"page\": i+1,\n",
    "            \"page_size\": page_size,\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "\n",
    "        singers = requests.get(root + \"chart.artists.get?\", params = param)\n",
    "        response = json.loads(singers.content)\n",
    "        artist_list = response.get(\"message\").get(\"body\").get(\"artist_list\")\n",
    "        \n",
    "        for artist in artist_list:\n",
    "            name = artist.get(\"artist\").get('artist_name')\n",
    "            genres = artist.get(\"artist\").get(\"primary_genres\").get(\"music_genre_list\")\n",
    "            for g in genres:\n",
    "                genre = g.get(\"music_genre\").get(\"music_genre_name\")\n",
    "                genre_id = g.get(\"music_genre\").get(\"music_genre_id\")\n",
    "                all_genres.add(genre)\n",
    "                result.append({\"artist\":name, \"genre\":genre, \"genre_id\":genre_id})\n",
    "    \n",
    "    df = pd.DataFrame(result)\n",
    "    df = df.loc[:, [\"artist\", \"genre\", \"genre_id\"]]\n",
    "    return df, all_genres\n",
    "\n",
    "\n",
    "def get_artist_genre(api, all_artist_list):\n",
    "\n",
    "    '''\n",
    "    getting the artists and their genres of given list\n",
    "\n",
    "    Args:\n",
    "        api: API key\n",
    "        all_artist_list: list of all the artists\n",
    "    Return:\n",
    "        df: a pandas dataframe containing artists, genres and genre id\n",
    "        all_genres: a set of all genres related to the artists found\n",
    "\n",
    "    '''\n",
    "    result = []\n",
    "    all_genres = set()\n",
    "    param = {\n",
    "            \"apikey\":api,\n",
    "            \"page\":1,\n",
    "            \"page_size\":10\n",
    "        }\n",
    "    for artist in all_artist_list:\n",
    "        param[\"q_artist\"] = artist\n",
    "\n",
    "        search_result = requests.get(root + \"artist.search?\", params = param)\n",
    "        response = json.loads(search_result.content)\n",
    "\n",
    "        artist_list = response.get(\"message\").get(\"body\").get(\"artist_list\")\n",
    "\n",
    "        if not artist_list:\n",
    "            continue\n",
    "        artist_item = artist_list[0]\n",
    "        \n",
    "        name = artist_item.get(\"artist\").get('artist_name')\n",
    "        genres = artist_item.get(\"artist\").get(\"primary_genres\").get(\"music_genre_list\")\n",
    "        for g in genres:\n",
    "            genre = g.get(\"music_genre\").get(\"music_genre_name\")\n",
    "            genre_id = g.get(\"music_genre\").get(\"music_genre_id\")\n",
    "            all_genres.add(genre)\n",
    "            result.append({\"artist\":name, \"genre\":genre, \"genre_id\":genre_id})\n",
    "    \n",
    "    df = pd.DataFrame(result)\n",
    "\n",
    "    if not df.empty:\n",
    "        df = df.loc[:, [\"artist\", \"genre\", \"genre_id\"]]\n",
    "    else:\n",
    "        print(\"result is an empty dataframe\")\n",
    "    return df, all_genres\n",
    "\n",
    "def get_songs(api, artist_df, page_size = 100):\n",
    "\n",
    "\n",
    "    '''\n",
    "    getting track names by artists and genre id\n",
    "\n",
    "    Args:\n",
    "        api: API key\n",
    "        artist_df: dataframe with columns of artist, genre and genre id\n",
    "        page_size: the page size for paginated results. Range is 1 to 100\n",
    "    Return:\n",
    "        df: a pandas dataframe containing artists, genres, genre id and the top\n",
    "        100 tracks with lyrics under that genre by the artist\n",
    "        \n",
    "    '''\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i, row in artist_df.iterrows(): \n",
    "        param = {\n",
    "                \"apikey\":api,\n",
    "                \"q_artist\": row['artist'],\n",
    "                \"f_music_genre_id\": row['genre_id'], # filter by genre id\n",
    "                \"f_has_lyrics\":\"True\", # only get tracks with lyrics\n",
    "                \"page\": 1,\n",
    "                \"page_size\": page_size\n",
    "            }\n",
    "\n",
    "        singer = requests.get(root + \"track.search?\", params = param)\n",
    "        response = json.loads(singer.content)\n",
    "        song_list = response.get(\"message\").get(\"body\").get(\"track_list\")\n",
    "        for song in song_list:\n",
    "    \n",
    "            track_name = song.get(\"track\").get(\"track_name\")\n",
    "            result.append(\n",
    "                {\n",
    "                \"artist\":row[\"artist\"], \n",
    "                \"genre\":row[\"genre\"], \n",
    "                \"genre_id\":row[\"genre_id\"],\n",
    "                \"track_name\":track_name\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    df = df.loc[:, [\"artist\", \"genre\",\"genre_id\", \"track_name\"]]\n",
    "    return df\n",
    "\n",
    "\n",
    "    #Step 1. get artists and their genres\n",
    "    # load the first 1,300 artists of from csv file\n",
    "    artist_df = pd.read_csv(\"./csv_files/all_female_artists.csv\", header = None)[:50]\n",
    "    artists_list = []\n",
    "    for col in artist_df.columns.values:\n",
    "        artists_list += list(artist_df[col])\n",
    "\n",
    "    artist_genre_df, all_genres = get_artist_genre(api, artists_list)\n",
    "    artist_genre_df.to_csv(\"./csv_files/all_female_artist_genre.csv\",index = False)\n",
    "\n",
    "    #Step 2. get songs by artists and genres\n",
    "    artist_df = pd.read_csv(\"./csv_files/all_female_artist_genre.csv\")[:1000]\n",
    "    print(artist_df.shape)\n",
    "    song_df = get_songs(api, artist_df)\n",
    "    song_df.to_csv(\"./csv_files/all_female_artist_genre_track.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Lyrics collection via *lyricwikia*\n",
    "\n",
    "With all the song names, we use [lyricwikia](https://github.com/enricobacis/lyricwikia) package in Python to collect the lyrics. The package can be installed with pip.\n",
    "\n",
    "```python\n",
    "pip3 install lyricwikia\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricwikia as ly\n",
    "\n",
    "#request lyric song by song\n",
    "#row by row in the dataframe\n",
    "def getLyrics(songs):\n",
    "    i = -1\n",
    "    print(\"Total songs number:\" + str(songs.shape[0]))\n",
    "    for index, row in songs.iterrows():\n",
    "        i += 1\n",
    "        if i%100 == 0:\n",
    "            print(\"Processing song [\" + str(i) + \"]\")\n",
    "\n",
    "        song = row['track_name']\n",
    "        #print(song)\n",
    "        artist = row['artist']\n",
    "        try:\n",
    "            lyric = ly.get_lyrics(artist, song, linesep='\\n', timeout=None)\n",
    "            songs.loc[index,'lyric'] = lyric\n",
    "        except:\n",
    "            continue    \n",
    "        #print(lyric)\n",
    "    return songs\n",
    "\n",
    "\n",
    "def run(oriFile, newFile):\n",
    "    songs = pd.read_csv(oriFile, encoding = \"ISO-8859-1\")\n",
    "    songs = getLyrics(songs)\n",
    "    songs = songs.dropna()\n",
    "    #print(songs)\n",
    "    songs.to_csv(newFile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run in command line:\n",
    "\n",
    "```bash\n",
    "python3 get_lyrics.py -h\n",
    "```\n",
    "to choose the csv file of the lyric track and customize the output path of the lyric file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the csv file of songs and their genres, we found the top 3 genres are:\n",
    "\n",
    "* Pop\n",
    "* Hip Hop/Rap\n",
    "* Rock\n",
    "\n",
    "We will train the model based on these three genres. Therefore, we will extract and generate the dataset of lyrics of each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_lyrics(csv_path):    \n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df = df.iloc[:,1:]\n",
    "\n",
    "    result = []\n",
    "    for genre in ['Pop','Rock','Hip Hop/Rap']:\n",
    "        result.append(df[df['genre'] == genre])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_female = split_lyrics('../csv_files/all_female_artist_lyrics.csv')\n",
    "    df_male = split_lyrics('../csv_files/all_male_artist_lyrics.csv')\n",
    "\n",
    "    for d, f in zip(df_female,df_male):\n",
    "        genre = d.iloc[0,1]\n",
    "        genre = genre.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "        df = pd.concat([d,f])\n",
    "        df.to_csv('../csv_files/lyrics_' + genre +\".csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train the model with 3 datasets consisting of lyrics of different genres. \n",
    "\n",
    "And the number of lyrics in each generes are:\n",
    "\n",
    "| genre | lyric number |\n",
    "|---|---|\n",
    "| rap | 5039 |\n",
    "| pop | 21998 |\n",
    "| rock | 6503 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The singer, song and lyric files are all stored in `../csv_files/` directory*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we'll apply two different deep learning methods:\n",
    "+ LSTM model to generate chosen genre of lyrics\n",
    "+ CNN model to classify a lyric into specific genre\n",
    "\n",
    "Since these two methods need different preprocessing, here we divide data preprocessing part into 2 part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Preprocessing for LSTM model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LSTM model, it is important to know the start and the end of a sentense. So here in the preprocessing stage, we manually add a start mark and an end mark to each lyric. And then use `nltk` package to tokenize lyrics into words and stem them. Then remove all the rare words.   <br>\n",
    "The most important method below is the `process` method, which generates all the features needed by the LSTM model. The returning X represents the word id sequences in each batch size lyrics. The Y is almost the same as X, except it is actually X moving 1 word to the right. And we also need a word to Id dict so that we can transform generated ids into words in the test stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "START_MARK = \"[\"\n",
    "END_MARK = \"]\"\n",
    "\n",
    "def seperate(docs_ls, is_rnn):\n",
    "    if is_rnn:\n",
    "        docs_raw = [tokenize(START_MARK+str(doc)+END_MARK) for doc in docs_ls]\n",
    "    else:\n",
    "        docs_raw = [tokenize(str(doc)) for doc in docs_ls]\n",
    "    docs = remove_stopwords(docs_raw)\n",
    "    print(\" \".join(docs[0]))\n",
    "    return docs\n",
    "\n",
    "def remove_stopwords(docs):\n",
    "    stopwords = get_rare_words(docs)\n",
    "    stopwords = set(stopwords)\n",
    "    res = [[word for word in doc if word not in stopwords ] for doc in docs]\n",
    "    return res\n",
    "\n",
    "\n",
    "def tokenize(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    text = text.replace(\"\\n\", \".\\n\")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    \n",
    "    punc = string.punctuation\n",
    "    for c in punc:\n",
    "        if c in text:\n",
    "            text = text.replace(c, ' '+c+' ')\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    res = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            word = lemmatizer.lemmatize(token)\n",
    "            res.append(str(word))\n",
    "        except:\n",
    "            continue\n",
    "    docs=nltk.word_tokenize(\" \".join(res))\n",
    "    return res\n",
    "\n",
    "def get_rare_words(tokens_ls):\n",
    "    \"\"\" use the word count information across all tweets in training data to come up with a feature list\n",
    "    Inputs:\n",
    "        processed_tweets: pd.DataFrame: the output of process_all() function\n",
    "    Outputs:\n",
    "        list(str): list of rare words, sorted alphabetically.\n",
    "    \"\"\"\n",
    "    counter = Counter([])\n",
    "    for tokens in tokens_ls:\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    rare_tokes = [k for k,v in counter.items() if v<=3]\n",
    "    rare_tokes.sort()\n",
    "    return rare_tokes\n",
    "\n",
    "def process(lyrics, batchSize=10, is_rnn=True):\n",
    "    \"\"\"\n",
    "    It will change lyrics to vetors as well as build the\n",
    "    features and labels for LSTM\n",
    "\n",
    "    lyric: list of str. all of the lyrics\n",
    "    return: (X, Y, vocab_size, vocab_ID, vocab)\n",
    "    \"\"\"\n",
    "\n",
    "    lyricDocs = seperate(lyrics, is_rnn)\n",
    "    print(\"Totally %d lyrics.\"%len(lyricDocs))\n",
    "\n",
    "    allWords = {}\n",
    "    for lyricDoc in lyricDocs:\n",
    "        for word in lyricDoc:\n",
    "            if word not in allWords:\n",
    "                allWords[word] = 1\n",
    "            else:\n",
    "                allWords[word] += 1\n",
    "\n",
    "    wordPairs = sorted(allWords.items(), key = lambda x: -x[1])\n",
    "    words, a= zip(*wordPairs)\n",
    "    words += (\" \", )\n",
    "    wordToID = dict(zip(words, range(len(words)))) #word to ID\n",
    "    wordTOIDFun = lambda A: wordToID.get(A, len(words))\n",
    "\n",
    "    lyricVector = [([wordTOIDFun(word) for word in lyricDoc]) for lyricDoc in lyricDocs] \n",
    "\n",
    "    batchNum = (len(lyrics) - 1) // batchSize \n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(batchNum):\n",
    "        batchVec = lyricVector[i*batchSize: (i+1)*batchSize]\n",
    "\n",
    "        maxLen = max([len(vector) for vector in batchVec])\n",
    "\n",
    "        temp = np.full((batchSize, maxLen), wordTOIDFun(\" \"),np.int32)\n",
    "\n",
    "        for j in range(batchSize):\n",
    "            temp[j, :len(batchVec[j])] = batchVec[j]\n",
    "\n",
    "        X.append(temp)\n",
    "\n",
    "        temp_copy = np.copy(temp)\n",
    "        temp_copy[:, :-1] = temp[:, 1:]\n",
    "        Y.append(temp_copy)\n",
    "    return X, Y, len(words) + 1, wordToID, words\n",
    "\n",
    "\n",
    "def generate_feature(filename, ouput_path):\n",
    "    \"\"\"\n",
    "    This methods is mainly for printing out the result to examine\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    docs = df['lyric'].values.tolist()[:100]\n",
    "    print(docs[0])\n",
    "    print()\n",
    "    print()\n",
    "    X, Y, size, wordToId, words = process(docs)\n",
    "    print(size)\n",
    "    print(X[0][0].shape)\n",
    "\n",
    "def pretreatment(filename, batchSize):\n",
    "    df = pd.read_csv(filename)\n",
    "    docs = df['lyric'].values\n",
    "    P = np.random.permutation(len(docs))\n",
    "    print(\"Shuffling\")\n",
    "    docs = docs[P].tolist()\n",
    "    print(\"Processing\")\n",
    "    return process(docs, batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the processing is a little bit time-consuming, we save the result into a pickle file to make it easier for us to testing LSTM model. We use the following code to save preprocessed LSTM data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(input_path, param_saving_path, batch_size):\n",
    "    X, Y, wordNum, wordToID, words = lyric_processing.pretreatment(input_path,batch_size)\n",
    "    data = {'X': X, \"Y\":Y, \"wordNum\":wordNum, \n",
    "        \"wordToID\": wordToID, \"words\":words, 'batch_size':batch_size}\n",
    "\n",
    "    with open(param_saving_path, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run in command line\n",
    "```bash\n",
    "python3 save_data.py -h\n",
    "```\n",
    "to choose the input raw data and customize your output file name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please Remember:**<br>\n",
    "The saved data is too big to upload to github, the pickle files in `./generate-param/` is only used for testing. If you want to do the training, you have to run `save_data.py` to generate needed pickle files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Preprocessing for CNN model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the LSTM model, the start and the end is not important in CNN model. <br>\n",
    "We use the same method in Part 1. to tokenize lyrics and generate a vocabulary. And then we pad each lyrics into the longest (or our chosen) length. (Here we pad using the mark `<PAD>`) Then we save parameters into a pickle file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lyric_processing import tokenize, remove_stopwords, seperate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as kr\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "filename_rock = \"../csv_files/lyrics_Rock.csv\"\n",
    "filename_pop  = \"../csv_files/lyrics_Pop.csv\"\n",
    "filename_rap  = \"../csv_files/lyrics_Hip_Hop_Rap.csv\"\n",
    "doc_num = 5000\n",
    "max_length = 800\n",
    "categories = ['pop','rock', 'rap']\n",
    "cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "\n",
    "\n",
    "def load_lyrics(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    docs = df['lyric'].values\n",
    "    return docs\n",
    "\n",
    "def get_raw_data():\n",
    "    # --------------- load and select data -------------\n",
    "    lyric_rock = load_lyrics(filename_rock)\n",
    "    lyric_pop = load_lyrics(filename_pop)\n",
    "    lyric_rap = load_lyrics(filename_rap)\n",
    "    P_rap = np.random.permutation(lyric_rap.shape[0])[:doc_num]\n",
    "    P_rock = np.random.permutation(lyric_rock.shape[0])[:doc_num]\n",
    "    P_pop = np.random.permutation(lyric_pop.shape[0])[:doc_num]\n",
    "\n",
    "    lyric_pop_chosen = lyric_pop[P_pop]\n",
    "    lyric_rap_chosen = lyric_rap[P_rap]\n",
    "    lyric_rock_chosen = lyric_rock[P_rock]\n",
    "    lyrics = np.concatenate((lyric_pop_chosen, lyric_rock_chosen, lyric_rap_chosen))\n",
    "\n",
    "    y_pop = np.array([cat_to_id['pop'] for _ in lyric_pop_chosen])\n",
    "    y_rock = np.array([cat_to_id['rock'] for _ in lyric_rock_chosen])\n",
    "    y_rap = np.array([cat_to_id['rap'] for _ in lyric_rap_chosen])\n",
    "    y = np.concatenate((y_pop, y_rock, y_rap))\n",
    "\n",
    "    return lyrics, y\n",
    "\n",
    "\n",
    "def process(param_saving_path):\n",
    "    lyrics, y = get_raw_data()\n",
    "    lyricDocs = seperate(lyrics, False)\n",
    "    print(\"Totally %d lyrics.\"%len(lyricDocs))\n",
    "    allWords = {}\n",
    "    for lyricDoc in lyricDocs:\n",
    "        for word in lyricDoc:\n",
    "            if word not in allWords:\n",
    "                allWords[word] = 1\n",
    "            else:\n",
    "                allWords[word] += 1\n",
    "\n",
    "    wordPairs = sorted(allWords.items(), key = lambda x: -x[1])\n",
    "    words, a= zip(*wordPairs)\n",
    "    words += (\" \", )\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    wordToID = dict(zip(words, range(len(words)))) #word to ID\n",
    "    wordTOIDFun = lambda A: wordToID.get(A, len(words))\n",
    "\n",
    "    lyricVector = [([wordTOIDFun(word) for word in lyricDoc]) for lyricDoc in lyricDocs]\n",
    "\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(lyricVector, max_length)\n",
    "    y_pad = kr.utils.to_categorical(y, num_classes=len(cat_to_id))\n",
    "\n",
    "    data = {'X': x_pad, 'Y': y_pad, 'wordToID': wordToID, 'seq_length': max_length}\n",
    "    \n",
    "    with open(param_saving_path, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print('Finish!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run in command line\n",
    "```bash\n",
    "python3 classification_preprocess.py -h\n",
    "```\n",
    "to customize your output parameter pickle file name and path. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please Remember:**<br>\n",
    "The saved data is too big to upload to github, the pickle files in `./generate-param/` is only used for testing. If you want to do the training, you have to run `classification_preprocess.py` to generate needed pickle files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Training Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. LSTM model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use LSTM model to generate lyrics for different genres. [This blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) clearly states the knowledge of LSTM. <br>\n",
    "For each genres, we do preprocessing and save the needed parameters into pickle files. Then we load the specific pickle file and train the LSTM model for the genre. <br>\n",
    "The word IDs will be embedded into a dense representation before feeding to the LSTM, which is called embedding layer. Here we use 2 layers of LSTM to process the data, followed by softmax representing each word's appearing probability. <br>\n",
    "In the training stage, we do 100 epoches. Since the data we use is very large and LSTM model is very slow to train. Here we use AWS to train 3 models for 3 genres. Even on AWS GPU server, it took nearly 40 hours to train. (40+ hours to train for pop, 18+ hours to train for rap and 10+ hours to train for rock). <br>\n",
    "The models are saved in:\n",
    "+ rap model:  `./checkpoints/rap/` \n",
    "+ rock model: `./checkpoints/rock/`\n",
    "+ pop model:  `./checkpoints/pop/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# batchSize = 10\n",
    "learningRateBase = 0.001\n",
    "learningRateDecreaseStep = 100\n",
    "epochNum = 100                    # train epoch\n",
    "\n",
    "generateNum = 1\n",
    "\n",
    "checkpointsPath = \"./checkpoints\" # checkpoints location\n",
    "\n",
    "def buildModel(wordNum, gtX, hidden_units = 128, layers = 2):\n",
    "    \"\"\"build rnn\"\"\"\n",
    "    with tf.variable_scope(\"embedding\"): #embedding\n",
    "        embedding = tf.get_variable(\"embedding\", [wordNum, hidden_units], dtype = tf.float32)\n",
    "        inputbatch = tf.nn.embedding_lookup(embedding, gtX)\n",
    "\n",
    "    basicCell = tf.contrib.rnn.BasicLSTMCell(hidden_units)    \n",
    "    stackCell = tf.contrib.rnn.MultiRNNCell([basicCell] * layers)\n",
    "    initState = stackCell.zero_state(np.shape(gtX)[0], tf.float32)\n",
    "    outputs, finalState = tf.nn.dynamic_rnn(stackCell, inputbatch, initial_state = initState)\n",
    "    outputs = tf.reshape(outputs, [-1, hidden_units])\n",
    "\n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        w = tf.get_variable(\"w\", [hidden_units, wordNum])\n",
    "        b = tf.get_variable(\"b\", [wordNum])\n",
    "        logits = tf.matmul(outputs, w) + b\n",
    "\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    return logits, probs, stackCell, initState, finalState\n",
    "\n",
    "def train(X, Y, wordNum, batchSize,reload=True):\n",
    "    \"\"\"train model\"\"\"\n",
    "    gtX = tf.placeholder(tf.int32, shape=[batchSize, None])  # input\n",
    "    gtY = tf.placeholder(tf.int32, shape=[batchSize, None])  # output\n",
    "    logits, probs, a, b, c = buildModel(wordNum, gtX)\n",
    "    targets = tf.reshape(gtY, [-1])\n",
    "    #loss\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [targets],\n",
    "                                                              [tf.ones_like(targets, dtype=tf.float32)], wordNum)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, a = tf.clip_by_global_norm(tf.gradients(cost, tvars), 5)\n",
    "    learningRate = learningRateBase\n",
    "    optimizer = tf.train.AdamOptimizer(learningRate)\n",
    "    trainOP = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    globalStep = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        if reload:\n",
    "            checkPoint = tf.train.get_checkpoint_state(checkpointsPath)\n",
    "            # if have checkPoint, restore checkPoint\n",
    "            if checkPoint and checkPoint.model_checkpoint_path:\n",
    "                saver.restore(sess, checkPoint.model_checkpoint_path)\n",
    "                print(\"restored %s\" % checkPoint.model_checkpoint_path)\n",
    "            else:\n",
    "                print(\"no checkpoint found!\")\n",
    "\n",
    "        for epoch in range(epochNum):\n",
    "            if globalStep % learningRateDecreaseStep == 0: #learning rate decrease by epoch\n",
    "                learningRate = learningRateBase * (0.95 ** epoch)\n",
    "            epochSteps = len(X) # equal to batch\n",
    "            for step, (x, y) in enumerate(zip(X, Y)):\n",
    "                globalStep = epoch * epochSteps + step\n",
    "                a, loss = sess.run([trainOP, cost], feed_dict = {gtX:x, gtY:y})\n",
    "                print(\"epoch: %d steps:%d/%d loss:%3f\" % (epoch,step,epochSteps,loss))\n",
    "                if globalStep%1000==0:\n",
    "                    print(\"save model\")\n",
    "                    # save_path = saver.save(sess, '/output/model.ckpt')\n",
    "                    save_path = saver.save(sess,checkpointsPath + \"/lyric\",global_step=epoch)\n",
    "                    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "def probsToWord(weights, words):\n",
    "    \"\"\"probs to word\"\"\"\n",
    "    t = np.cumsum(weights) #prefix sum\n",
    "    s = np.sum(weights)\n",
    "    coff = np.random.rand(1)\n",
    "    index = int(np.searchsorted(t, coff * s)) # large margin has high possibility to be sampled\n",
    "    return words[index]\n",
    "\n",
    "def test(wordNum, wordToID, words, model_path=checkpointsPath):\n",
    "    \"\"\"generate lyric\"\"\"\n",
    "    gtX = tf.placeholder(tf.int32, shape=[1, None])  # input\n",
    "    logits, probs, stackCell, initState, finalState = buildModel(wordNum, gtX)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        checkPoint = tf.train.get_checkpoint_state(model_path)\n",
    "        # if have checkPoint, restore checkPoint\n",
    "        if checkPoint and checkPoint.model_checkpoint_path:\n",
    "            print(checkPoint.model_checkpoint_path)\n",
    "            saver.restore(sess, checkPoint.model_checkpoint_path)\n",
    "            print(\"restored %s\" % checkPoint.model_checkpoint_path)\n",
    "            print(\"\\n\\n\")\n",
    "        else:\n",
    "            print(\"no checkpoint found!\")\n",
    "            exit(0)\n",
    "\n",
    "        lyrics = []\n",
    "        for i in range(generateNum):\n",
    "            state = sess.run(stackCell.zero_state(1, tf.float32))\n",
    "            x = np.array([[wordToID['[']]]) # init start sign\n",
    "            probs1, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n",
    "            word = probsToWord(probs1, words)\n",
    "            lyric = ''\n",
    "            while word != ']' and word != ' ':\n",
    "                if word == '.':\n",
    "                    try:\n",
    "                        if not (lyric[-1]=='.' and lyric[-2] == '.'):\n",
    "                            lyric += '. '\n",
    "                    except:\n",
    "                        pass\n",
    "                else:\n",
    "                    lyric += word + ' '\n",
    "                x = np.array([[wordToID[word]]])\n",
    "                #print(word)\n",
    "                probs2, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n",
    "                word = probsToWord(probs2, words)\n",
    "            print(\"The generated lyrics: \\n\")\n",
    "            print(lyric.replace(\". \", \"\\n\"))\n",
    "            lyrics.append(lyric)\n",
    "        return lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run in command line\n",
    "```bash\n",
    "python3 main.py -h\n",
    "```\n",
    "to choose training or testing the LSTM mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the method mentioned in [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) <br>\n",
    "The architecture of the model is listed as below, which is taken from the above article. \n",
    "![CNN model](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Example-of-a-CNN-Filter-and-Polling-Architecture-for-Natural-Language-Processing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I set the word embedding dimension to be 600 and each sequence length to be 800. (If not satisfied we add `<PAD>` in the front. ) We choose 256 convolution filters and each size is 5 followed by a max-over-time polling. Then we use a fully connected layers with drop out and ReLU. And finally use softmax to do the classification. (Here we do 3-class classification: the 3 genres mentioned above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import tensorflow.contrib.keras as kr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import argparse\n",
    "from classification_preprocess import cat_to_id\n",
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "\n",
    "\n",
    "save_dir = './checkpoints/textcnn'\n",
    "save_path = os.path.join(save_dir, 'best_validation')\n",
    "param_saving_path = '../data/param-classify.dat'\n",
    "tensorboard_dir = './tensorboard/textcnn'\n",
    "validation_rate = 0.1\n",
    "\n",
    "class TCNNConfig(object):\n",
    "    \"\"\"CNN param\"\"\"\n",
    "    embedding_dim = 64  # word vector dimension\n",
    "    seq_length = 800  # sequense length\n",
    "    num_classes = 3  # class number\n",
    "    num_filters = 256  # kernel number\n",
    "    kernel_size = 5  # kernel size\n",
    "    vocab_size = 5000  # vocab size\n",
    "\n",
    "    hidden_dim = 128  # fully connected neuro number\n",
    "\n",
    "    dropout_keep_prob = 0.5  # dropout keeping rate\n",
    "    learning_rate = 1e-3  # learning rate\n",
    "\n",
    "    batch_size = 64  # batch size\n",
    "    num_epochs = 10  # total epoch number\n",
    "\n",
    "    print_per_batch = 10  # output iterations\n",
    "    save_per_batch = 10  # save tensorboard iterations\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"text classification，CNN model\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        self.cnn()\n",
    "\n",
    "    def cnn(self):\n",
    "        \"\"\"CNN model\"\"\"\n",
    "        # word embedding\n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim])\n",
    "            embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            # CNN layer\n",
    "            conv = tf.layers.conv1d(embedding_inputs, self.config.num_filters, self.config.kernel_size, name='conv')\n",
    "            # global max pooling layer\n",
    "            gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')\n",
    "\n",
    "        with tf.name_scope(\"score\"):\n",
    "            # fully connected layer，with dropout and ReLU\n",
    "            fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "\n",
    "            # classifier\n",
    "            self.logits = tf.layers.dense(fc, self.config.num_classes, name='fc2')\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # predictor\n",
    "\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            # loss function，cross entropy\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            # optimizor\n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # accuracy\n",
    "            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"get time\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "def batch_iter(x, y, batch_size=64):\n",
    "    \"\"\"generate batchsize data\"\"\"\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indices]\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n",
    "\n",
    "def feed_data(model, x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def evaluate(model, sess, x_, y_):\n",
    "    \"\"\"evaluate the loss and accuracy\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(model, x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "def train(filename):\n",
    "    config = TCNNConfig()\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    x = data['X']\n",
    "    y = data['Y']\n",
    "    print(len(x))\n",
    "    P = np.random.permutation(len(x))\n",
    "    x = x[P]\n",
    "    y = y[P]\n",
    "\n",
    "    wordToID = data['wordToID']\n",
    "    seq_length = data['seq_length']\n",
    "    config.vocab_size = len(wordToID)\n",
    "    config.seq_length = seq_length\n",
    "\n",
    "    model = TextCNN(config)\n",
    "\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "    \n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    idx = int(x.shape[0] * validation_rate)\n",
    "    x_train = x[idx:]\n",
    "    x_val = x[:idx]\n",
    "    y_train = y[idx:]\n",
    "    y_val = y[:idx]\n",
    "    \n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(session.graph)\n",
    "    \n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # total batch number\n",
    "    best_acc_val = 0.0  # best validation accuracy\n",
    "    last_improved = 0  # last improving\n",
    "    require_improvement = 1000  # if not improving after 1000 iterations, end early\n",
    "    \n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            feed_dict = feed_data(model, x_batch, y_batch, config.dropout_keep_prob)\n",
    "            \n",
    "            if total_batch % config.save_per_batch == 0:\n",
    "                # save to tensorboard scalar\n",
    "                s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # get the loss and accuracy on training set and validation set\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                loss_val, acc_val = evaluate(model, session, x_val, y_val)  # todo\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # save the best result\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    print(\"Save model!\")\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>4.4}, Train Acc: {2:>5.2%},' \\\n",
    "                      + ' Val Loss: {3:>4.4}, Val Acc: {4:>5.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "            session.run(model.optim, feed_dict=feed_dict)  \n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # early end\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break \n",
    "        if flag:\n",
    "            break\n",
    "\n",
    "\n",
    "def test(text, filename, genre, model_path=save_dir):\n",
    "\n",
    "    config = TCNNConfig()\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    wordToID = data['wordToID']\n",
    "    seq_length = data['seq_length']\n",
    "    config.vocab_size = len(wordToID)\n",
    "    config.seq_length = seq_length\n",
    "\n",
    "    model = TextCNN(config)\n",
    "\n",
    "    text_ids = [[wordToID[word] for word in text.split(\" \") if word in wordToID]]\n",
    "    # print(text_ids)\n",
    "    y = np.array([cat_to_id[genre]])\n",
    "\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(text_ids, seq_length)\n",
    "    y_pad = kr.utils.to_categorical(y, num_classes=len(cat_to_id)) \n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        checkPoint = tf.train.get_checkpoint_state(model_path)\n",
    "        # if have checkPoint, restore checkPoint\n",
    "        if checkPoint and checkPoint.model_checkpoint_path:\n",
    "            print(checkPoint.model_checkpoint_path)\n",
    "            saver.restore(session, checkPoint.model_checkpoint_path)\n",
    "            print(\"restored %s\" % checkPoint.model_checkpoint_path)\n",
    "            print(\"\\n\\n\")\n",
    "        else:\n",
    "            print(\"no checkpoint found!\")\n",
    "            exit(0)\n",
    "\n",
    "        \n",
    "\n",
    "        print('Testing...')\n",
    "\n",
    "        feed_dict = feed_data(model, x_pad, y_pad, 1.0)\n",
    "        y_pred = session.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "        return list(cat_to_id)[y_pred[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training processing is in `classification.ipynb`, where prints out the details in training. <br>\n",
    "You can also run in command line\n",
    "```bash\n",
    "python3 classification_model.py -h\n",
    "```\n",
    "to choose training or testing mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss figure is:\n",
    "![Screen Shot 2018-05-07 at 9.25.54 PM](https://oh1ulkf4j.qnssl.com/Screen Shot 2018-05-07 at 9.25.54 PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the training accuracy figure is:\n",
    "![Screen Shot 2018-05-07 at 9.25.40 PM](https://oh1ulkf4j.qnssl.com/Screen Shot 2018-05-07 at 9.25.40 PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figures are recorded by tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saving model's accuracy on testing dataset is 77.40%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is saved in `./checkpoints/textcnn/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 4. Sentiment analysis\n",
    "To analyze the sentiment of some text, do an HTTP POST to http://text-processing.com/api/sentiment/ with form encoded data containing the text we want to analyze. (1000 requests per IP a day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "URL = \"http://text-processing.com/api/sentiment/\"\n",
    "prefix = \"text=\"\n",
    "\n",
    "def get_sentiment(text):\n",
    "    param = prefix + text\n",
    "    response = requests.post(URL, param)\n",
    "    res = response.json()\n",
    "    pos_dict = res['probability']\n",
    "    label = res['label']\n",
    "    print(\"The label is: \" + label)\n",
    "    print(\"The positive possibility is: %f\"%pos_dict[\"pos\"])\n",
    "    print(\"The negative possibility is: %f\"%pos_dict[\"neg\"])\n",
    "    print(\"The netural possibility is: %f\"%pos_dict[\"neutral\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run in command line:\n",
    "```bash\n",
    "python3 sentiment_analysis.py -h \n",
    "```\n",
    "to test an input sentense's sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 5. Display Result (It's show time!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use AWS server to train 3 LSTM lyric generator models for 3 genres and train the CNN classification model locally. With those saving models, now we can use our LSTM model to generate lyric in chosen genre. And then use our CNN classification model to test the result. Finally do a sentiment analysis for the generated lyric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get lyric in random, instead of selecting the word with the highest probability, I map the probability to an interval and randomly sample one. See in `probsToWord` method in Step3 part1. (Of course each lyric starts with the starting mark `[`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are recommended to run in command line:\n",
    "```bash\n",
    "python3 generator.py -g [pop/rock/rap]\n",
    "```\n",
    "to generate a chosen genre lyric and verify in our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from model import test as generate_model\n",
    "from classification_model import test as classify_model\n",
    "import tensorflow as tf\n",
    "from sentiment_analysis import get_sentiment\n",
    "\n",
    "pop_model = \"./checkpoints/pop\"\n",
    "pop_save = \"./generate-param/param-pop-10-test.dat\"\n",
    "\n",
    "rock_model = \"./checkpoints/rock\"\n",
    "rock_save = \"./generate-param/param-rock-10-test.dat\"\n",
    "\n",
    "rap_model = \"./checkpoints/rap\"\n",
    "rap_save = \"./generate-param/param-rap-10-test.dat\"\n",
    "\n",
    "classify_model_path = \"./checkpoints/textcnn\"\n",
    "classify_save = \"./generate-param/param-classify-test.dat\"\n",
    "\n",
    "\n",
    "\n",
    "def run(genre):\n",
    "    if genre == 'pop':\n",
    "        model_path = pop_model\n",
    "        data_path = pop_save\n",
    "    elif genre == 'rock':\n",
    "        model_path = rock_model\n",
    "        data_path = rock_save\n",
    "    elif genre == 'rap':\n",
    "        model_path = rap_model\n",
    "        data_path = rap_save\n",
    "    else:\n",
    "        print(\"Unexpected input!\")\n",
    "\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print('generating...')\n",
    "\n",
    "    lyrics = generate_model(data['wordNum'], \n",
    "        data['wordToID'], \n",
    "        data['words'], \n",
    "        model_path=model_path)\n",
    "\n",
    "    print('\\n\\n')\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    predicted = classify_model(lyrics[0], classify_save, genre, model_path=classify_model_path)\n",
    "    print(\"\\n\\nOur classification model predict it to be: \")\n",
    "    print(predicted)\n",
    "    \n",
    "    print(\"\\nAnd the sentiment analysis result of the generated lyric is:\")\n",
    "    get_sentiment(lyrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Pop lyric and verify\n",
    "We now use the 'Pop' LSTM model to generate a pop lyric. And verify the result with our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n",
      "./checkpoints/pop/lyric-99\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/pop/lyric-99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/pop/lyric-99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored ./checkpoints/pop/lyric-99\n",
      "\n",
      "\n",
      "\n",
      "The generated lyrics: \n",
      "\n",
      "i knew all your dream \n",
      "you would give me everything \n",
      "ill make you feel \n",
      "ball kinda scene \n",
      "everything is inside \n",
      "well , it all that were fighting for \n",
      "driving into this bad zip me feeling \n",
      "\n",
      "sometimes im when i know \n",
      "seeing someone , then talk about me \n",
      "boy , knew \n",
      "stop tip and elvis ? \n",
      "that you got on someone crazy guy \n",
      "call me \n",
      "me , , think problem \n",
      "\n",
      "sometimes i make sense to you \n",
      "i know it getting colder \n",
      "i cant break away for you \n",
      "there are other baby ( ever , baby ) \n",
      "( oh , i said what she want , hey ) \n",
      "hey baby , baby baby \n",
      "( ill na all because you cry ) \n",
      "anything love you \n",
      "cause i cant make you proud \n",
      ", im over you \n",
      "aint been so unavailable \n",
      "\n",
      "{ run on n girl ? \n",
      "\n",
      "im game yo i know that you ridin solo \n",
      "oh , im walking on the ground \n",
      "oh , cause im about to be hurt \n",
      "oh i got to lose you \n",
      "\n",
      "oh , for when i walk a second \n",
      "why should i be witness \n",
      "i just know that i want to know \n",
      "didnt tell you , forgive me \n",
      "oh , your dad didnt \n",
      "id do your body when i should be around \n",
      "oh , when it ok come out tonight \n",
      "\n",
      "i want to let you go \n",
      "ooh baby , babe \n",
      "i know that we are all right \n",
      "ive learned i knock , they say you mean my song \n",
      "when it take up \n",
      "im so sorry if i tried it too late \n",
      "until youre back then \n",
      "\n",
      "youve been thinkin , youre all soon , i cant complain \n",
      "why do you think it wouldnt be quite bad \n",
      "kicking the rooftop , cover me on table for good \n",
      "now get on upstairs , i been waitin from here when i know the word you got \n",
      "my heart keep doing so hard \n",
      "say the soon time i can tell me \n",
      "\n",
      "everybody know you know that everythings better place \n",
      "we go to work \n",
      "you know im ready to think all you got \n",
      "it just never right \n",
      "im looking from what \n",
      "it been good like you \n",
      "if you never let me go \n",
      "whyd you have to try it \n",
      "late in you ) \n",
      "why do i ? say ? \n",
      "better , better \n",
      "tell me \n",
      "about it , then you care \n",
      "i get caught \n",
      "and i know like my pain \n",
      "real good enough to miss me ( oh ) \n",
      "ooh \n",
      "what about this \n",
      "and ill be having nobody right \n",
      "now whatll i do it ) until i play \n",
      "( oh oh ) ( ever gon na touch ya ) \n",
      "i know you take me from the end \n",
      "because im all alone \n",
      "for every time she came back to me ( so long ) \n",
      "i want to be my friend \n",
      "just a shot like i knew myself forever \n",
      "that i , babe , but when love feel all \n",
      "and i know your name \n",
      "baby get ten touch ( uh oh ) \n",
      "again ( ooh ) \n",
      "( ( come away ) \n",
      "\n",
      "baby , somethin about you to do \n",
      "( excuse me , yeah ) \n",
      "ya said `` love hard 2 matter ? \n",
      "( so much in love ) \n",
      "just trying to be a cure \n",
      "guess all thats been that we got ta be \n",
      "\n",
      "girl we could not be sad just what it about \n",
      "what you wan na do , do what you love like how you do the deal \n",
      "cause im mad at some touch you , before , ah give me \n",
      "let me be the one , but baby , boy , yeah \n",
      "\n",
      "cause it , way but mine girl , girl \n",
      "repeat want a want to give ur everything for a second time \n",
      "so i long for a long way out \n",
      "yeah it so bad \n",
      "so i do the same ooh \n",
      "\n",
      "\n",
      "\n",
      "./checkpoints/textcnn/best_validation\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/textcnn/best_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/textcnn/best_validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored ./checkpoints/textcnn/best_validation\n",
      "\n",
      "\n",
      "\n",
      "Testing...\n",
      "\n",
      "\n",
      "Our classification model predict it to be: \n",
      "pop\n",
      "\n",
      "And the sentiment analysis result of the generated lyric is:\n",
      "The label is: neg\n",
      "The positive possibility is: 0.245676\n",
      "The negative possibility is: 0.754324\n",
      "The netural possibility is: 0.486980\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "run(\"pop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a rock lyric and verify\n",
    "We now use the 'rock' LSTM model to generate a rock lyric. And verify the result with our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n",
      "./checkpoints/rock/lyric-98\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/rock/lyric-98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/rock/lyric-98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored ./checkpoints/rock/lyric-98\n",
      "\n",
      "\n",
      "\n",
      "The generated lyrics: \n",
      "\n",
      "were scared and her guitar waiting bright \n",
      "at u not to tame me \n",
      "golden word they say an thief \n",
      "dead men aint played \n",
      "everybody know were now no music in \n",
      "\n",
      "time can take ourselves to the beat \n",
      "and breathe on the fire \n",
      "set it all right \n",
      "\n",
      "or paid for friend \n",
      "when our chance is going down \n",
      "to find the way that i need \n",
      "the greatest life in a long way \n",
      "to lay back from crack \n",
      "somewhere off the finish line \n",
      "\n",
      "sometimes youre home \n",
      "distracted by the point \n",
      "yearning like johnny like a \n",
      "cause dream of something real man \n",
      "live with no one who searching without you \n",
      "do you please enjoy it ? \n",
      "\n",
      "aint a motion and a different dress \n",
      "what can we say ? \n",
      "\n",
      "\n",
      "they dont have to say \n",
      "you go so far behind \n",
      "but it honestly \n",
      "but there aint the sum \n",
      "to control this \n",
      "better enough for not who care \n",
      "and tomorrow and only a word \n",
      "now im sure we are \n",
      "it wa dancing and we are \n",
      "with you and you and we \n",
      "\n",
      "i dont walk away \n",
      "never ending we taking double disappointment \n",
      "maybe how it failed to sell them know \n",
      "to do true \n",
      "who dont believe with it \n",
      "\n",
      "good time are closing time \n",
      "seven distance and million year \n",
      "i lost the weight of you can breathe \n",
      "one journey nine year \n",
      "it youll never let them fall \n",
      "but to make me glorious \n",
      "and the kid is gone \n",
      "but now you who want to scream \n",
      "within you \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "./checkpoints/textcnn/best_validation\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/textcnn/best_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/textcnn/best_validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored ./checkpoints/textcnn/best_validation\n",
      "\n",
      "\n",
      "\n",
      "Testing...\n",
      "\n",
      "\n",
      "Our classification model predict it to be: \n",
      "rock\n",
      "\n",
      "And the sentiment analysis result of the generated lyric is:\n",
      "The label is: neg\n",
      "The positive possibility is: 0.207900\n",
      "The negative possibility is: 0.792100\n",
      "The netural possibility is: 0.343937\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "run(\"rock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Rap lyric and verify\n",
    "We now use the 'rap' LSTM model to generate a rap lyric. And verify the result with our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n",
      "./checkpoints/rap/lyric-99\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/rap/lyric-99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/rap/lyric-99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored ./checkpoints/rap/lyric-99\n",
      "\n",
      "\n",
      "\n",
      "The generated lyrics: \n",
      "\n",
      "i woke up in your spot to drown on the top \n",
      "rise from mr \n",
      "burn that place up , just feel so easy \n",
      "and my worst will do all of the flow im just proud before \n",
      "daisy , oh oh oh \n",
      "\n",
      "remember everybody were yearning for , be on where next jewel \n",
      "the new friend are wack , ive been peeped run , we can feel in this shoe on in gravel \n",
      "im writing the light like the pawn shop or i saw her underneath you moanin \n",
      "\n",
      "( cmon ) my word are often a , then pas this end of woman \n",
      "and i lack on swim with myself cologne in the momma with his palm \n",
      "my hat come his head , cause i wa very much name ? yeah im gon see gimmick to me \n",
      "to aid with you and \n",
      "they act like he wa \n",
      "be trying to navigate and my nephew cry , laugh , \n",
      "i make me a pitchfork and im the getting the blue so you can bet i be cool , and if god nigga say , thats just not nice , did it kick , \n",
      "i got no flaw \n",
      "and when i leave , i saw a tear fire on you , i wouldnt touch god im \n",
      "im dreamin to see cassette and sunshine , like we wa way train \n",
      "throughout my city \n",
      "\n",
      "and im a rebel coming to puttin on which of , that mean that of eye on me \n",
      "and i wa all started , tryna bring it that be up \n",
      "i realize what did i askin what happened to a snotty and smile \n",
      "stay with to and i eat \n",
      "\n",
      "rule toe to earth when you find a voice to drive \n",
      "like weed knock when it off that thing \n",
      "of robin expression pitched nine in it \n",
      "and grab up when we get - out in the stream , kat \n",
      "if i gave you in the studio , you just have that i want \n",
      "we wa free for myself to be in the skyline \n",
      "for for my people ( i dont know ) feel a body for you ( oh ) \n",
      "i love that as feeling - aw right \n",
      "`` im all right , right back `` \n",
      "you know i am from the \n",
      "i know it not a my chick , i aint sorry \n",
      "thats my pain \n",
      "\n",
      "i let her out my cell phone , six - 6 \n",
      "bitch what bad is zombie \n",
      "l \n",
      "two \n",
      "only if im living time \n",
      "wan na let it ride , show you this forever \n",
      "( feleti ) \n",
      "be that far , no one in the system you wont forget \n",
      "so complain that you should save her \n",
      "i said everybody are the little man tonight \n",
      "till they speak and this is what we say about u \n",
      "we certified it \n",
      "and on the , ill see kd \n",
      "\n",
      "boss , all around live , see one of time i look right \n",
      "thats why you cant wait to make it out every city a theyll have a state \n",
      "and nothing is not no change , but got to let him out a lonely star \n",
      "wa the first second but wa soft , so instead of the show \n",
      "in a even named , took with those message want a peace \n",
      "\n",
      "forced to dance , no one gon na jump up , front is knowing my friendship from , they like to bring it out \n",
      "tell you , i let it do , im like im so , i aint selfish \n",
      "\n",
      "make everything go alive , i know all the sign that \n",
      "you dont want with me show , man , on me \n",
      "they get the camera on the floor \n",
      "\n",
      "( patti ) : \n",
      "why cant often overcome again sweetie , i guess i can \n",
      "do it for the people , im takin this \n",
      "with the whole life so take your life and die \n",
      "{ mr \n",
      "maxwell always limit } \n",
      "baby i wa insane , so i cant love the truth \n",
      "i promise yes im checkin all my people \n",
      "i love you to me in that city there for you \n",
      "there what you got more than it is to it , for me \n",
      "so bring it over \n",
      "\n",
      "i am mr \n",
      "lonely \n",
      "\n",
      "\n",
      "\n",
      "./checkpoints/textcnn/best_validation\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/textcnn/best_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/textcnn/best_validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored ./checkpoints/textcnn/best_validation\n",
      "\n",
      "\n",
      "\n",
      "Testing...\n",
      "\n",
      "\n",
      "Our classification model predict it to be: \n",
      "rap\n",
      "\n",
      "And the sentiment analysis result of the generated lyric is:\n",
      "The label is: neg\n",
      "The positive possibility is: 0.272287\n",
      "The negative possibility is: 0.727713\n",
      "The netural possibility is: 0.478927\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "run(\"rap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
