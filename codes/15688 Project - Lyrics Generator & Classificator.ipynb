{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15688 Project - Lyrics Generator & Classifier\n",
    "\n",
    "“Music can change the word because it can change people.” said by the Legendary U2 rocker Bono. A beautiful song usually has memorable lyrics that sometimes change people. However, it is not an easy task to write good lyrics. \n",
    "\n",
    "The aim of our project is to create a lyric generation model based on existing lyrics of different music genres - pop, rock, hip hop, etc - using machine learning algorithms that are common in natural language processing.\n",
    "\n",
    "We will use LSTM model to generate the lyrics of difference music genres, and CNN model as the lyric classificator. \n",
    "\n",
    "If you want to go through all the whole project, you can go to [this github repository](https://github.com/Htiango/Lyrics-Generator), clone it to your local and run according to our instruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention:** <br>\n",
    "We develop this project via python scripts instead of Jupyter Notebook. <br>\n",
    "This tutorial mainly walks you through each step.  <br>\n",
    "It is highly recommended to go over this project via the command line we suggest instead of on this notebook. (In case some unexpected error). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data Collection\n",
    "\n",
    "### Part 1. Singer and song collection\n",
    "In order to train the lyric model, the first step is to collect lyrics by genre. We collected the male and female artists' names from [music.163.com ](https://music.163.com/#/discover/artist/cat?id=2001)by copying the information on the webpage and saved them as csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting artists' names, we use the [musixmatch](http://api.musixmatch.com/ws/1.1/) api to collect the genre and name of songs of the artists. The results are exported as csv file so that we can count the most frequent genres among all the songs we collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "15688 final project - lyric generator\n",
    "\n",
    "data collection\n",
    "\n",
    "retrieve the artists, genres and tracks and export to csv file\n",
    "\n",
    "API used: musixmatch Developer\n",
    "documentation: https://developer.musixmatch.com/documentation\n",
    "\n",
    "'''\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# load api key\n",
    "with open(\"../musicmatch_api.key\",'r') as f:\n",
    "    api = f.read()\n",
    "\n",
    "root = \"http://api.musixmatch.com/ws/1.1/\"\n",
    "\n",
    "def get_artist(api, pageNum, page_size=100, country = \"us\"):\n",
    "\n",
    "    '''\n",
    "    getting top artists and their genres\n",
    "    Args:\n",
    "        api: API key\n",
    "        pageNum: the page number for paginated results\n",
    "        page_size: the page size for paginated results. Range is 1 to 100\n",
    "        country: country of the artist ranking\n",
    "    Return:\n",
    "        df: a pandas dataframe containing artists, genres and genre id\n",
    "        all_genres: a set of all genres related to the artists found\n",
    "    '''\n",
    "    result = []\n",
    "    all_genres = set()\n",
    "    for i in range(pageNum):\n",
    "        param = {\n",
    "            \"apikey\":api,\n",
    "            \"country\": \"country\",\n",
    "            \"page\": i+1,\n",
    "            \"page_size\": page_size,\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "\n",
    "        singers = requests.get(root + \"chart.artists.get?\", params = param)\n",
    "        response = json.loads(singers.content)\n",
    "        artist_list = response.get(\"message\").get(\"body\").get(\"artist_list\")\n",
    "        \n",
    "        for artist in artist_list:\n",
    "            name = artist.get(\"artist\").get('artist_name')\n",
    "            genres = artist.get(\"artist\").get(\"primary_genres\").get(\"music_genre_list\")\n",
    "            for g in genres:\n",
    "                genre = g.get(\"music_genre\").get(\"music_genre_name\")\n",
    "                genre_id = g.get(\"music_genre\").get(\"music_genre_id\")\n",
    "                all_genres.add(genre)\n",
    "                result.append({\"artist\":name, \"genre\":genre, \"genre_id\":genre_id})\n",
    "    \n",
    "    df = pd.DataFrame(result)\n",
    "    df = df.loc[:, [\"artist\", \"genre\", \"genre_id\"]]\n",
    "    return df, all_genres\n",
    "\n",
    "\n",
    "def get_artist_genre(api, all_artist_list):\n",
    "\n",
    "    '''\n",
    "    getting the artists and their genres of given list\n",
    "\n",
    "    Args:\n",
    "        api: API key\n",
    "        all_artist_list: list of all the artists\n",
    "    Return:\n",
    "        df: a pandas dataframe containing artists, genres and genre id\n",
    "        all_genres: a set of all genres related to the artists found\n",
    "\n",
    "    '''\n",
    "    result = []\n",
    "    all_genres = set()\n",
    "    param = {\n",
    "            \"apikey\":api,\n",
    "            \"page\":1,\n",
    "            \"page_size\":10\n",
    "        }\n",
    "    for artist in all_artist_list:\n",
    "        param[\"q_artist\"] = artist\n",
    "\n",
    "        search_result = requests.get(root + \"artist.search?\", params = param)\n",
    "        response = json.loads(search_result.content)\n",
    "\n",
    "        artist_list = response.get(\"message\").get(\"body\").get(\"artist_list\")\n",
    "\n",
    "        if not artist_list:\n",
    "            continue\n",
    "        artist_item = artist_list[0]\n",
    "        \n",
    "        name = artist_item.get(\"artist\").get('artist_name')\n",
    "        genres = artist_item.get(\"artist\").get(\"primary_genres\").get(\"music_genre_list\")\n",
    "        for g in genres:\n",
    "            genre = g.get(\"music_genre\").get(\"music_genre_name\")\n",
    "            genre_id = g.get(\"music_genre\").get(\"music_genre_id\")\n",
    "            all_genres.add(genre)\n",
    "            result.append({\"artist\":name, \"genre\":genre, \"genre_id\":genre_id})\n",
    "    \n",
    "    df = pd.DataFrame(result)\n",
    "\n",
    "    if not df.empty:\n",
    "        df = df.loc[:, [\"artist\", \"genre\", \"genre_id\"]]\n",
    "    else:\n",
    "        print(\"result is an empty dataframe\")\n",
    "    return df, all_genres\n",
    "\n",
    "def get_songs(api, artist_df, page_size = 100):\n",
    "\n",
    "\n",
    "    '''\n",
    "    getting track names by artists and genre id\n",
    "\n",
    "    Args:\n",
    "        api: API key\n",
    "        artist_df: dataframe with columns of artist, genre and genre id\n",
    "        page_size: the page size for paginated results. Range is 1 to 100\n",
    "    Return:\n",
    "        df: a pandas dataframe containing artists, genres, genre id and the top\n",
    "        100 tracks with lyrics under that genre by the artist\n",
    "        \n",
    "    '''\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i, row in artist_df.iterrows(): \n",
    "        param = {\n",
    "                \"apikey\":api,\n",
    "                \"q_artist\": row['artist'],\n",
    "                \"f_music_genre_id\": row['genre_id'], # filter by genre id\n",
    "                \"f_has_lyrics\":\"True\", # only get tracks with lyrics\n",
    "                \"page\": 1,\n",
    "                \"page_size\": page_size\n",
    "            }\n",
    "\n",
    "        singer = requests.get(root + \"track.search?\", params = param)\n",
    "        response = json.loads(singer.content)\n",
    "        song_list = response.get(\"message\").get(\"body\").get(\"track_list\")\n",
    "        for song in song_list:\n",
    "    \n",
    "            track_name = song.get(\"track\").get(\"track_name\")\n",
    "            result.append(\n",
    "                {\n",
    "                \"artist\":row[\"artist\"], \n",
    "                \"genre\":row[\"genre\"], \n",
    "                \"genre_id\":row[\"genre_id\"],\n",
    "                \"track_name\":track_name\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    df = df.loc[:, [\"artist\", \"genre\",\"genre_id\", \"track_name\"]]\n",
    "    return df\n",
    "\n",
    "\n",
    "#Step 1. get artists and their genres\n",
    "# load the first 1,300 artists of from csv file\n",
    "artist_df = pd.read_csv(\"./csv_files/all_female_artists.csv\", header = None)[:50]\n",
    "artists_list = []\n",
    "for col in artist_df.columns.values:\n",
    "    artists_list += list(artist_df[col])\n",
    "\n",
    "artist_genre_df, all_genres = get_artist_genre(api, artists_list)\n",
    "artist_genre_df.to_csv(\"./csv_files/all_female_artist_genre.csv\",index = False)\n",
    "\n",
    "#Step 2. get songs by artists and genres\n",
    "artist_df = pd.read_csv(\"./csv_files/all_female_artist_genre.csv\")[:1000]\n",
    "print(artist_df.shape)\n",
    "song_df = get_songs(api, artist_df)\n",
    "song_df.to_csv(\"./csv_files/all_female_artist_genre_track.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Lyrics collection via *lyricwikia*\n",
    "\n",
    "With all the song names, we use [lyricwikia](https://github.com/enricobacis/lyricwikia) package in Python to collect the lyrics. The package can be installed with pip.\n",
    "\n",
    "```python\n",
    "pip3 install lyricwikia\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricwikia as ly\n",
    "\n",
    "#request lyric song by song\n",
    "#row by row in the dataframe\n",
    "def getLyrics(songs):\n",
    "    i = -1\n",
    "    print(\"Total songs number:\" + str(songs.shape[0]))\n",
    "    for index, row in songs.iterrows():\n",
    "        i += 1\n",
    "        if i%100 == 0:\n",
    "            print(\"Processing song [\" + str(i) + \"]\")\n",
    "\n",
    "        song = row['track_name']\n",
    "        #print(song)\n",
    "        artist = row['artist']\n",
    "        try:\n",
    "            lyric = ly.get_lyrics(artist, song, linesep='\\n', timeout=None)\n",
    "            songs.loc[index,'lyric'] = lyric\n",
    "        except:\n",
    "            continue    \n",
    "        #print(lyric)\n",
    "    return songs\n",
    "\n",
    "\n",
    "def run(oriFile, newFile):\n",
    "    songs = pd.read_csv(oriFile, encoding = \"ISO-8859-1\")\n",
    "    songs = getLyrics(songs)\n",
    "    songs = songs.dropna()\n",
    "    #print(songs)\n",
    "    songs.to_csv(newFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run in command line:\n",
    "\n",
    "```bash\n",
    "python3 get_lyrics.py -h\n",
    "```\n",
    "to choose the csv file of the lyric track and customize the output path of the lyric file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the csv file of songs and their genres, we found the top 3 genres are:\n",
    "\n",
    "* Pop\n",
    "* Hip Hop/Rap\n",
    "* Rock\n",
    "\n",
    "We will train the model based on these three genres. Therefore, we will extract and generate the dataset of lyrics of each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_lyrics(csv_path):\n",
    "    '''\n",
    "    load the csv dataset of lyrics and select the data of desired genres.\n",
    "    Args:\n",
    "        csv_path: the file path of the original file.\n",
    "    \n",
    "    '''\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df = df.iloc[:,1:]\n",
    "\n",
    "    result = []\n",
    "    for genre in ['Pop','Rock','Hip Hop/Rap']:\n",
    "        result.append(df[df['genre'] == genre])\n",
    "    return result\n",
    "\n",
    "df_female = split_lyrics('../csv_files/all_female_artist_lyrics.csv')\n",
    "df_male = split_lyrics('../csv_files/all_male_artist_lyrics.csv')\n",
    "\n",
    "for d, f in zip(df_female,df_male):\n",
    "    genre = d.iloc[0,1]\n",
    "    genre = genre.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    df = pd.concat([d,f])\n",
    "    df.to_csv('../csv_files/lyrics_' + genre +\".csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of lyrics in each generes are:\n",
    "\n",
    "| genre | lyric number |\n",
    "|---|---|\n",
    "| rap | 5039 |\n",
    "| pop | 21998 |\n",
    "| rock | 6503 | \n",
    "\n",
    "We are now ready to train the model with 3 datasets consisting of lyrics of different genres. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The singer, song and lyric files are all stored in `../csv_files/` directory*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we'll apply two different deep learning methods:\n",
    "+ LSTM model to generate chosen genre of lyrics\n",
    "+ CNN model to classify a lyric into specific genre\n",
    "\n",
    "Since these two methods need different preprocessing, here we divide data preprocessing part into 2 part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Preprocessing for LSTM model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LSTM model, it is important to know the start and the end of a sentense. So here in the preprocessing stage, we manually add a start mark and an end mark to each lyric. And then use `nltk` package to tokenize lyrics into words and stem them. Then remove all the rare words.   <br>\n",
    "The most important method below is the `process` method, which generates all the features needed by the LSTM model. The returning X represents the word id sequences in each batch size lyrics. The Y is almost the same as X, except it is actually X moving 1 word to the right. And we also need a word to Id dict so that we can transform generated ids into words in the test stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "START_MARK = \"[\"\n",
    "END_MARK = \"]\"\n",
    "\n",
    "def seperate(docs_ls, is_rnn):\n",
    "    if is_rnn:\n",
    "        docs_raw = [tokenize(START_MARK+str(doc)+END_MARK) for doc in docs_ls]\n",
    "    else:\n",
    "        docs_raw = [tokenize(str(doc)) for doc in docs_ls]\n",
    "    docs = remove_stopwords(docs_raw)\n",
    "    print(\" \".join(docs[0]))\n",
    "    return docs\n",
    "\n",
    "def remove_stopwords(docs):\n",
    "    stopwords = get_rare_words(docs)\n",
    "    stopwords = set(stopwords)\n",
    "    res = [[word for word in doc if word not in stopwords ] for doc in docs]\n",
    "    return res\n",
    "\n",
    "\n",
    "def tokenize(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    text = text.replace(\"\\n\", \".\\n\")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    \n",
    "    punc = string.punctuation\n",
    "    for c in punc:\n",
    "        if c in text:\n",
    "            text = text.replace(c, ' '+c+' ')\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    res = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            word = lemmatizer.lemmatize(token)\n",
    "            res.append(str(word))\n",
    "        except:\n",
    "            continue\n",
    "    docs=nltk.word_tokenize(\" \".join(res))\n",
    "    return res\n",
    "\n",
    "def get_rare_words(tokens_ls):\n",
    "    \"\"\" use the word count information across all tweets in training data to come up with a feature list\n",
    "    Inputs:\n",
    "        processed_tweets: pd.DataFrame: the output of process_all() function\n",
    "    Outputs:\n",
    "        list(str): list of rare words, sorted alphabetically.\n",
    "    \"\"\"\n",
    "    counter = Counter([])\n",
    "    for tokens in tokens_ls:\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    rare_tokes = [k for k,v in counter.items() if v<=3]\n",
    "    rare_tokes.sort()\n",
    "    return rare_tokes\n",
    "\n",
    "def process(lyrics, batchSize=10, is_rnn=True):\n",
    "    \"\"\"\n",
    "    It will change lyrics to vetors as well as build the\n",
    "    features and labels for LSTM\n",
    "\n",
    "    lyric: list of str. all of the lyrics\n",
    "    return: (X, Y, vocab_size, vocab_ID, vocab)\n",
    "    \"\"\"\n",
    "\n",
    "    lyricDocs = seperate(lyrics, is_rnn)\n",
    "    print(\"Totally %d lyrics.\"%len(lyricDocs))\n",
    "\n",
    "    allWords = {}\n",
    "    for lyricDoc in lyricDocs:\n",
    "        for word in lyricDoc:\n",
    "            if word not in allWords:\n",
    "                allWords[word] = 1\n",
    "            else:\n",
    "                allWords[word] += 1\n",
    "\n",
    "    wordPairs = sorted(allWords.items(), key = lambda x: -x[1])\n",
    "    words, a= zip(*wordPairs)\n",
    "    words += (\" \", )\n",
    "    wordToID = dict(zip(words, range(len(words)))) #word to ID\n",
    "    wordTOIDFun = lambda A: wordToID.get(A, len(words))\n",
    "\n",
    "    lyricVector = [([wordTOIDFun(word) for word in lyricDoc]) for lyricDoc in lyricDocs] \n",
    "\n",
    "    batchNum = (len(lyrics) - 1) // batchSize \n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(batchNum):\n",
    "        batchVec = lyricVector[i*batchSize: (i+1)*batchSize]\n",
    "\n",
    "        maxLen = max([len(vector) for vector in batchVec])\n",
    "\n",
    "        temp = np.full((batchSize, maxLen), wordTOIDFun(\" \"),np.int32)\n",
    "\n",
    "        for j in range(batchSize):\n",
    "            temp[j, :len(batchVec[j])] = batchVec[j]\n",
    "\n",
    "        X.append(temp)\n",
    "\n",
    "        temp_copy = np.copy(temp)\n",
    "        temp_copy[:, :-1] = temp[:, 1:]\n",
    "        Y.append(temp_copy)\n",
    "    return X, Y, len(words) + 1, wordToID, words\n",
    "\n",
    "\n",
    "def generate_feature(filename, ouput_path):\n",
    "    \"\"\"\n",
    "    This methods is mainly for printing out the result to examine\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    docs = df['lyric'].values.tolist()[:100]\n",
    "    print(docs[0])\n",
    "    print()\n",
    "    print()\n",
    "    X, Y, size, wordToId, words = process(docs)\n",
    "    print(size)\n",
    "    print(X[0][0].shape)\n",
    "\n",
    "def pretreatment(filename, batchSize):\n",
    "    df = pd.read_csv(filename)\n",
    "    docs = df['lyric'].values\n",
    "    P = np.random.permutation(len(docs))\n",
    "    print(\"Shuffling\")\n",
    "    docs = docs[P].tolist()\n",
    "    print(\"Processing\")\n",
    "    return process(docs, batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the processing is a little bit time-consuming, we save the result into a pickle file to make it easier for us to testing LSTM model. We use the following code to save preprocessed LSTM data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(input_path, param_saving_path, batch_size):\n",
    "    X, Y, wordNum, wordToID, words = lyric_processing.pretreatment(input_path,batch_size)\n",
    "    data = {'X': X, \"Y\":Y, \"wordNum\":wordNum, \n",
    "        \"wordToID\": wordToID, \"words\":words, 'batch_size':batch_size}\n",
    "\n",
    "    with open(param_saving_path, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run in command line\n",
    "```bash\n",
    "python3 save_data.py -h\n",
    "```\n",
    "to choose the input raw data and customize your output file name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please Remember:**<br>\n",
    "The saved data is too big to upload to github, the pickle files in `./generate-param/` is only used for testing. If you want to do the training, you have to run `save_data.py` to generate needed pickle files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Preprocessing for CNN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the LSTM model, the start and the end is not important in CNN model. <br>\n",
    "We use the same method in Part 1. to tokenize lyrics and generate a vocabulary. And then we pad each lyrics into the longest (or our chosen) length. (Here we pad using the mark `<PAD>`) Then we save parameters into a pickle file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lyric_processing import tokenize, remove_stopwords, seperate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as kr\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "filename_rock = \"../csv_files/lyrics_Rock.csv\"\n",
    "filename_pop  = \"../csv_files/lyrics_Pop.csv\"\n",
    "filename_rap  = \"../csv_files/lyrics_Hip_Hop_Rap.csv\"\n",
    "doc_num = 5000\n",
    "max_length = 800\n",
    "categories = ['pop','rock', 'rap']\n",
    "cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "\n",
    "\n",
    "def load_lyrics(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    docs = df['lyric'].values\n",
    "    return docs\n",
    "\n",
    "def get_raw_data():\n",
    "    # --------------- load and select data -------------\n",
    "    lyric_rock = load_lyrics(filename_rock)\n",
    "    lyric_pop = load_lyrics(filename_pop)\n",
    "    lyric_rap = load_lyrics(filename_rap)\n",
    "    P_rap = np.random.permutation(lyric_rap.shape[0])[:doc_num]\n",
    "    P_rock = np.random.permutation(lyric_rock.shape[0])[:doc_num]\n",
    "    P_pop = np.random.permutation(lyric_pop.shape[0])[:doc_num]\n",
    "\n",
    "    lyric_pop_chosen = lyric_pop[P_pop]\n",
    "    lyric_rap_chosen = lyric_rap[P_rap]\n",
    "    lyric_rock_chosen = lyric_rock[P_rock]\n",
    "    lyrics = np.concatenate((lyric_pop_chosen, lyric_rock_chosen, lyric_rap_chosen))\n",
    "\n",
    "    y_pop = np.array([cat_to_id['pop'] for _ in lyric_pop_chosen])\n",
    "    y_rock = np.array([cat_to_id['rock'] for _ in lyric_rock_chosen])\n",
    "    y_rap = np.array([cat_to_id['rap'] for _ in lyric_rap_chosen])\n",
    "    y = np.concatenate((y_pop, y_rock, y_rap))\n",
    "\n",
    "    return lyrics, y\n",
    "\n",
    "\n",
    "def process(param_saving_path):\n",
    "    lyrics, y = get_raw_data()\n",
    "    lyricDocs = seperate(lyrics, False)\n",
    "    print(\"Totally %d lyrics.\"%len(lyricDocs))\n",
    "    allWords = {}\n",
    "    for lyricDoc in lyricDocs:\n",
    "        for word in lyricDoc:\n",
    "            if word not in allWords:\n",
    "                allWords[word] = 1\n",
    "            else:\n",
    "                allWords[word] += 1\n",
    "\n",
    "    wordPairs = sorted(allWords.items(), key = lambda x: -x[1])\n",
    "    words, a= zip(*wordPairs)\n",
    "    words += (\" \", )\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    wordToID = dict(zip(words, range(len(words)))) #word to ID\n",
    "    wordTOIDFun = lambda A: wordToID.get(A, len(words))\n",
    "\n",
    "    lyricVector = [([wordTOIDFun(word) for word in lyricDoc]) for lyricDoc in lyricDocs]\n",
    "\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(lyricVector, max_length)\n",
    "    y_pad = kr.utils.to_categorical(y, num_classes=len(cat_to_id))\n",
    "\n",
    "    data = {'X': x_pad, 'Y': y_pad, 'wordToID': wordToID, 'seq_length': max_length}\n",
    "    \n",
    "    with open(param_saving_path, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print('Finish!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run in command line\n",
    "```bash\n",
    "python3 classification_preprocess.py -h\n",
    "```\n",
    "to customize your output parameter pickle file name and path. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please Remember:**<br>\n",
    "The saved data is too big to upload to github, the pickle files in `./generate-param/` is only used for testing. If you want to do the training, you have to run `classification_preprocess.py` to generate needed pickle files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Training Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. LSTM model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use LSTM model to generate lyrics for different genres. [This blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) clearly states the knowledge of LSTM. <br>\n",
    "For each genres, we do preprocessing and save the needed parameters into pickle files. Then we load the specific pickle file and train the LSTM model for the genre. <br>\n",
    "The architecture of the LSTM model is shown below:\n",
    "![LSTM-model-2](https://oh1ulkf4j.qnssl.com/LSTM-model-2.jpg)<br>\n",
    "The word IDs will be embedded into a dense representation before feeding to the LSTM, which is called embedding layer. Here we use 2 layers of LSTM to process the data, followed by softmax representing each word's appearing probability. <br>\n",
    "In the training stage, we do 100 epoches. Since the data we use is very large and LSTM model is very slow to train. Here we use AWS to train 3 models for 3 genres. Even on AWS GPU server, it took nearly 40 hours to train. (40+ hours to train for pop, 18+ hours to train for rap and 10+ hours to train for rock). <br>\n",
    "The models are saved in:\n",
    "+ rap model:  `./checkpoints/rap/` \n",
    "+ rock model: `./checkpoints/rock/`\n",
    "+ pop model:  `./checkpoints/pop/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# batchSize = 10\n",
    "learningRateBase = 0.001\n",
    "learningRateDecreaseStep = 100\n",
    "epochNum = 100                    # train epoch\n",
    "\n",
    "generateNum = 1\n",
    "\n",
    "checkpointsPath = \"./checkpoints\" # checkpoints location\n",
    "\n",
    "def buildModel(wordNum, gtX, hidden_units = 128, layers = 2):\n",
    "    \"\"\"build rnn\"\"\"\n",
    "    with tf.variable_scope(\"embedding\"): #embedding\n",
    "        embedding = tf.get_variable(\"embedding\", [wordNum, hidden_units], dtype = tf.float32)\n",
    "        inputbatch = tf.nn.embedding_lookup(embedding, gtX)\n",
    "\n",
    "    basicCell = tf.contrib.rnn.BasicLSTMCell(hidden_units)    \n",
    "    stackCell = tf.contrib.rnn.MultiRNNCell([basicCell] * layers)\n",
    "    initState = stackCell.zero_state(np.shape(gtX)[0], tf.float32)\n",
    "    outputs, finalState = tf.nn.dynamic_rnn(stackCell, inputbatch, initial_state = initState)\n",
    "    outputs = tf.reshape(outputs, [-1, hidden_units])\n",
    "\n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        w = tf.get_variable(\"w\", [hidden_units, wordNum])\n",
    "        b = tf.get_variable(\"b\", [wordNum])\n",
    "        logits = tf.matmul(outputs, w) + b\n",
    "\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    return logits, probs, stackCell, initState, finalState\n",
    "\n",
    "def train(X, Y, wordNum, batchSize,reload=True):\n",
    "    \"\"\"train model\"\"\"\n",
    "    gtX = tf.placeholder(tf.int32, shape=[batchSize, None])  # input\n",
    "    gtY = tf.placeholder(tf.int32, shape=[batchSize, None])  # output\n",
    "    logits, probs, a, b, c = buildModel(wordNum, gtX)\n",
    "    targets = tf.reshape(gtY, [-1])\n",
    "    #loss\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [targets],\n",
    "                                                              [tf.ones_like(targets, dtype=tf.float32)], wordNum)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, a = tf.clip_by_global_norm(tf.gradients(cost, tvars), 5)\n",
    "    learningRate = learningRateBase\n",
    "    optimizer = tf.train.AdamOptimizer(learningRate)\n",
    "    trainOP = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    globalStep = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        if reload:\n",
    "            checkPoint = tf.train.get_checkpoint_state(checkpointsPath)\n",
    "            # if have checkPoint, restore checkPoint\n",
    "            if checkPoint and checkPoint.model_checkpoint_path:\n",
    "                saver.restore(sess, checkPoint.model_checkpoint_path)\n",
    "                print(\"restored %s\" % checkPoint.model_checkpoint_path)\n",
    "            else:\n",
    "                print(\"no checkpoint found!\")\n",
    "\n",
    "        for epoch in range(epochNum):\n",
    "            if globalStep % learningRateDecreaseStep == 0: #learning rate decrease by epoch\n",
    "                learningRate = learningRateBase * (0.95 ** epoch)\n",
    "            epochSteps = len(X) # equal to batch\n",
    "            for step, (x, y) in enumerate(zip(X, Y)):\n",
    "                globalStep = epoch * epochSteps + step\n",
    "                a, loss = sess.run([trainOP, cost], feed_dict = {gtX:x, gtY:y})\n",
    "                print(\"epoch: %d steps:%d/%d loss:%3f\" % (epoch,step,epochSteps,loss))\n",
    "                if globalStep%1000==0:\n",
    "                    print(\"save model\")\n",
    "                    # save_path = saver.save(sess, '/output/model.ckpt')\n",
    "                    save_path = saver.save(sess,checkpointsPath + \"/lyric\",global_step=epoch)\n",
    "                    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "def probsToWord(weights, words):\n",
    "    \"\"\"probs to word\"\"\"\n",
    "    t = np.cumsum(weights) #prefix sum\n",
    "    s = np.sum(weights)\n",
    "    coff = np.random.rand(1)\n",
    "    index = int(np.searchsorted(t, coff * s)) # large margin has high possibility to be sampled\n",
    "    return words[index]\n",
    "\n",
    "def test(wordNum, wordToID, words, model_path=checkpointsPath):\n",
    "    \"\"\"generate lyric\"\"\"\n",
    "    gtX = tf.placeholder(tf.int32, shape=[1, None])  # input\n",
    "    logits, probs, stackCell, initState, finalState = buildModel(wordNum, gtX)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        checkPoint = tf.train.get_checkpoint_state(model_path)\n",
    "        # if have checkPoint, restore checkPoint\n",
    "        if checkPoint and checkPoint.model_checkpoint_path:\n",
    "            print(checkPoint.model_checkpoint_path)\n",
    "            saver.restore(sess, checkPoint.model_checkpoint_path)\n",
    "            print(\"restored %s\" % checkPoint.model_checkpoint_path)\n",
    "            print(\"\\n\\n\")\n",
    "        else:\n",
    "            print(\"no checkpoint found!\")\n",
    "            exit(0)\n",
    "\n",
    "        lyrics = []\n",
    "        for i in range(generateNum):\n",
    "            state = sess.run(stackCell.zero_state(1, tf.float32))\n",
    "            x = np.array([[wordToID['[']]]) # init start sign\n",
    "            probs1, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n",
    "            word = probsToWord(probs1, words)\n",
    "            lyric = ''\n",
    "            while word != ']' and word != ' ':\n",
    "                if word == '.':\n",
    "                    try:\n",
    "                        if not (lyric[-1]=='.' and lyric[-2] == '.'):\n",
    "                            lyric += '. '\n",
    "                    except:\n",
    "                        pass\n",
    "                else:\n",
    "                    lyric += word + ' '\n",
    "                x = np.array([[wordToID[word]]])\n",
    "                #print(word)\n",
    "                probs2, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n",
    "                word = probsToWord(probs2, words)\n",
    "            print(\"The generated lyrics: \\n\")\n",
    "            print(lyric.replace(\". \", \"\\n\"))\n",
    "            lyrics.append(lyric)\n",
    "        return lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run in command line\n",
    "```bash\n",
    "python3 main.py -h\n",
    "```\n",
    "to choose training or testing the LSTM mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the method mentioned in [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) <br>\n",
    "The architecture of the model is listed as below, which is taken from the above article. \n",
    "![CNN model](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Example-of-a-CNN-Filter-and-Polling-Architecture-for-Natural-Language-Processing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I set the word embedding dimension to be 600 and each sequence length to be 800. (If not satisfied we add `<PAD>` in the front. ) We choose 256 convolution filters and each size is 5 followed by a max-over-time polling. Then we use a fully connected layers with drop out and ReLU. And finally use softmax to do the classification. (Here we do 3-class classification: the 3 genres mentioned above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import tensorflow.contrib.keras as kr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import argparse\n",
    "from classification_preprocess import cat_to_id\n",
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "\n",
    "\n",
    "save_dir = './checkpoints/textcnn'\n",
    "save_path = os.path.join(save_dir, 'best_validation')\n",
    "param_saving_path = '../data/param-classify.dat'\n",
    "tensorboard_dir = './tensorboard/textcnn'\n",
    "validation_rate = 0.1\n",
    "\n",
    "class TCNNConfig(object):\n",
    "    \"\"\"CNN param\"\"\"\n",
    "    embedding_dim = 64  # word vector dimension\n",
    "    seq_length = 800  # sequense length\n",
    "    num_classes = 3  # class number\n",
    "    num_filters = 256  # kernel number\n",
    "    kernel_size = 5  # kernel size\n",
    "    vocab_size = 5000  # vocab size\n",
    "\n",
    "    hidden_dim = 128  # fully connected neuro number\n",
    "\n",
    "    dropout_keep_prob = 0.5  # dropout keeping rate\n",
    "    learning_rate = 1e-3  # learning rate\n",
    "\n",
    "    batch_size = 64  # batch size\n",
    "    num_epochs = 10  # total epoch number\n",
    "\n",
    "    print_per_batch = 10  # output iterations\n",
    "    save_per_batch = 10  # save tensorboard iterations\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"text classification，CNN model\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        self.cnn()\n",
    "\n",
    "    def cnn(self):\n",
    "        \"\"\"CNN model\"\"\"\n",
    "        # word embedding\n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim])\n",
    "            embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            # CNN layer\n",
    "            conv = tf.layers.conv1d(embedding_inputs, self.config.num_filters, self.config.kernel_size, name='conv')\n",
    "            # global max pooling layer\n",
    "            gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')\n",
    "\n",
    "        with tf.name_scope(\"score\"):\n",
    "            # fully connected layer，with dropout and ReLU\n",
    "            fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "\n",
    "            # classifier\n",
    "            self.logits = tf.layers.dense(fc, self.config.num_classes, name='fc2')\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # predictor\n",
    "\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            # loss function，cross entropy\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            # optimizor\n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # accuracy\n",
    "            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"get time\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "def batch_iter(x, y, batch_size=64):\n",
    "    \"\"\"generate batchsize data\"\"\"\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indices]\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n",
    "\n",
    "def feed_data(model, x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def evaluate(model, sess, x_, y_):\n",
    "    \"\"\"evaluate the loss and accuracy\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(model, x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "def train(filename):\n",
    "    config = TCNNConfig()\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    x = data['X']\n",
    "    y = data['Y']\n",
    "    print(len(x))\n",
    "    P = np.random.permutation(len(x))\n",
    "    x = x[P]\n",
    "    y = y[P]\n",
    "\n",
    "    wordToID = data['wordToID']\n",
    "    seq_length = data['seq_length']\n",
    "    config.vocab_size = len(wordToID)\n",
    "    config.seq_length = seq_length\n",
    "\n",
    "    model = TextCNN(config)\n",
    "\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "    \n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    idx = int(x.shape[0] * validation_rate)\n",
    "    x_train = x[idx:]\n",
    "    x_val = x[:idx]\n",
    "    y_train = y[idx:]\n",
    "    y_val = y[:idx]\n",
    "    \n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(session.graph)\n",
    "    \n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # total batch number\n",
    "    best_acc_val = 0.0  # best validation accuracy\n",
    "    last_improved = 0  # last improving\n",
    "    require_improvement = 1000  # if not improving after 1000 iterations, end early\n",
    "    \n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            feed_dict = feed_data(model, x_batch, y_batch, config.dropout_keep_prob)\n",
    "            \n",
    "            if total_batch % config.save_per_batch == 0:\n",
    "                # save to tensorboard scalar\n",
    "                s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # get the loss and accuracy on training set and validation set\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                loss_val, acc_val = evaluate(model, session, x_val, y_val)  # todo\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # save the best result\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    print(\"Save model!\")\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>4.4}, Train Acc: {2:>5.2%},' \\\n",
    "                      + ' Val Loss: {3:>4.4}, Val Acc: {4:>5.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "            session.run(model.optim, feed_dict=feed_dict)  \n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # early end\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break \n",
    "        if flag:\n",
    "            break\n",
    "\n",
    "\n",
    "def test(text, filename, genre, model_path=save_dir):\n",
    "\n",
    "    config = TCNNConfig()\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    wordToID = data['wordToID']\n",
    "    seq_length = data['seq_length']\n",
    "    config.vocab_size = len(wordToID)\n",
    "    config.seq_length = seq_length\n",
    "\n",
    "    model = TextCNN(config)\n",
    "\n",
    "    text_ids = [[wordToID[word] for word in text.split(\" \") if word in wordToID]]\n",
    "    # print(text_ids)\n",
    "    y = np.array([cat_to_id[genre]])\n",
    "\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(text_ids, seq_length)\n",
    "    y_pad = kr.utils.to_categorical(y, num_classes=len(cat_to_id)) \n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        checkPoint = tf.train.get_checkpoint_state(model_path)\n",
    "        # if have checkPoint, restore checkPoint\n",
    "        if checkPoint and checkPoint.model_checkpoint_path:\n",
    "            print(checkPoint.model_checkpoint_path)\n",
    "            saver.restore(session, checkPoint.model_checkpoint_path)\n",
    "            print(\"restored %s\" % checkPoint.model_checkpoint_path)\n",
    "            print(\"\\n\\n\")\n",
    "        else:\n",
    "            print(\"no checkpoint found!\")\n",
    "            exit(0)\n",
    "\n",
    "        \n",
    "\n",
    "        print('Testing...')\n",
    "\n",
    "        feed_dict = feed_data(model, x_pad, y_pad, 1.0)\n",
    "        y_pred = session.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "        return list(cat_to_id)[y_pred[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training processing is in `classification.ipynb`, where prints out the details in training. <br>\n",
    "You can also run in command line\n",
    "```bash\n",
    "python3 classification_model.py -h\n",
    "```\n",
    "to choose training or testing mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss figure is:\n",
    "![Screen Shot 2018-05-07 at 9.25.54 PM](https://oh1ulkf4j.qnssl.com/Screen Shot 2018-05-07 at 9.25.54 PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the training accuracy figure is:\n",
    "![Screen Shot 2018-05-07 at 9.25.40 PM](https://oh1ulkf4j.qnssl.com/Screen Shot 2018-05-07 at 9.25.40 PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figures are recorded by tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saving model's accuracy on testing dataset is 77.40%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is saved in `./checkpoints/textcnn/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 4. Sentiment analysis\n",
    "To analyze the sentiment of some text, do an HTTP POST to http://text-processing.com/api/sentiment/ with form encoded data containing the text we want to analyze. (1000 requests per IP a day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "URL = \"http://text-processing.com/api/sentiment/\"\n",
    "prefix = \"text=\"\n",
    "\n",
    "def get_sentiment(text):\n",
    "    param = prefix + text\n",
    "    response = requests.post(URL, param)\n",
    "    res = response.json()\n",
    "    pos_dict = res['probability']\n",
    "    label = res['label']\n",
    "    print(\"The label is: \" + label)\n",
    "    print(\"The positive possibility is: %f\"%pos_dict[\"pos\"])\n",
    "    print(\"The negative possibility is: %f\"%pos_dict[\"neg\"])\n",
    "    print(\"The netural possibility is: %f\"%pos_dict[\"neutral\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run in command line:\n",
    "```bash\n",
    "python3 sentiment_analysis.py -h \n",
    "```\n",
    "to test an input sentense's sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 5. Display Result (It's show time!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use AWS server to train 3 LSTM lyric generator models for 3 genres and train the CNN classification model locally. With those saving models, now we can use our LSTM model to generate lyric in chosen genre. And then use our CNN classification model to test the result. Finally do a sentiment analysis for the generated lyric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get lyric in random, instead of selecting the word with the highest probability, I map the probability to an interval and randomly sample one. See in `probsToWord` method in Step3 part1. (Of course each lyric starts with the starting mark `[`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are recommended to run in command line:\n",
    "```bash\n",
    "python3 generator.py -g [pop/rock/rap]\n",
    "```\n",
    "to generate a chosen genre lyric and verify in our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from model import test as generate_model\n",
    "from classification_model import test as classify_model\n",
    "import tensorflow as tf\n",
    "from sentiment_analysis import get_sentiment\n",
    "\n",
    "pop_model = \"./checkpoints/pop\"\n",
    "pop_save = \"./generate-param/param-pop-10-test.dat\"\n",
    "\n",
    "rock_model = \"./checkpoints/rock\"\n",
    "rock_save = \"./generate-param/param-rock-10-test.dat\"\n",
    "\n",
    "rap_model = \"./checkpoints/rap\"\n",
    "rap_save = \"./generate-param/param-rap-10-test.dat\"\n",
    "\n",
    "classify_model_path = \"./checkpoints/textcnn\"\n",
    "classify_save = \"./generate-param/param-classify-test.dat\"\n",
    "\n",
    "\n",
    "\n",
    "def run(genre):\n",
    "    if genre == 'pop':\n",
    "        model_path = pop_model\n",
    "        data_path = pop_save\n",
    "    elif genre == 'rock':\n",
    "        model_path = rock_model\n",
    "        data_path = rock_save\n",
    "    elif genre == 'rap':\n",
    "        model_path = rap_model\n",
    "        data_path = rap_save\n",
    "    else:\n",
    "        print(\"Unexpected input!\")\n",
    "\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print('generating...')\n",
    "\n",
    "    lyrics = generate_model(data['wordNum'], \n",
    "        data['wordToID'], \n",
    "        data['words'], \n",
    "        model_path=model_path)\n",
    "\n",
    "    print('\\n\\n')\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    predicted = classify_model(lyrics[0], classify_save, genre, model_path=classify_model_path)\n",
    "    print(\"\\n\\nOur classification model predict it to be: \")\n",
    "    print(predicted)\n",
    "    \n",
    "    print(\"\\nAnd the sentiment analysis result of the generated lyric is:\")\n",
    "    get_sentiment(lyrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Pop lyric and verify\n",
    "We now use the 'Pop' LSTM model to generate a pop lyric. And verify the result with our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n",
      "./checkpoints/pop/lyric-99\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/pop/lyric-99\n",
      "restored ./checkpoints/pop/lyric-99\n",
      "\n",
      "\n",
      "\n",
      "The generated lyrics: \n",
      "\n",
      "feel like ive been dancing alive \n",
      "just want to be there wondrous to winter \n",
      "under december , in the night \n",
      "i see a silence like a soul \n",
      "will see the wind in a tree , im drifting \n",
      "in love through my canvas , the last cloud are here \n",
      "turning the light , bursting through \n",
      "to light and light \n",
      "and to my every little , ill make you smile \n",
      "after my soul , or prove to be right ; with you and i , \n",
      "still i know because i want to argue , with the sun \n",
      "\n",
      "your love so free and stay with your way , \n",
      "all our need is ending together \n",
      "\n",
      "\n",
      "\n",
      "through the night , with you thats all out of these year , \n",
      "to save somebody who doe me at you \n",
      "\n",
      "you look by your side , still to bleed \n",
      "is there no sleep ? \n",
      "somewhere together we can dream from all it \n",
      "\n",
      "\n",
      "\n",
      "the angel in your eye i lay with you \n",
      "attracted to a feather \n",
      "sweet and the only red hand \n",
      "to let you know \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "./checkpoints/textcnn/best_validation\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/textcnn/best_validation\n",
      "restored ./checkpoints/textcnn/best_validation\n",
      "\n",
      "\n",
      "\n",
      "Testing...\n",
      "\n",
      "\n",
      "Our classification model predict it to be: \n",
      "pop\n",
      "\n",
      "And the sentiment analysis result of the generated lyric is:\n",
      "The label is: pos\n",
      "The positive possibility is: 0.651010\n",
      "The negative possibility is: 0.348990\n",
      "The netural possibility is: 0.157258\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "run(\"pop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a rock lyric and verify\n",
    "We now use the 'rock' LSTM model to generate a rock lyric. And verify the result with our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n",
      "./checkpoints/rock/lyric-98\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/rock/lyric-98\n",
      "restored ./checkpoints/rock/lyric-98\n",
      "\n",
      "\n",
      "\n",
      "The generated lyrics: \n",
      "\n",
      "i loved her girl and who fell away \n",
      "im the feeling gave out after all \n",
      "with your tear and the sky came bright night time \n",
      "it took such a a fresh a the sky that you could fall \n",
      "\n",
      "\n",
      "call a little bag \n",
      "and when you smile \n",
      "ill hold you for you , cry baby , your hurt \n",
      "and every time you just wont go \n",
      "if i love you old man \n",
      "you better weep your name \n",
      "but i dont whisper all \n",
      "though id rather be my friend , i think you got ta let me go \n",
      "\n",
      "so let me want you \n",
      "\n",
      "\n",
      "\n",
      "./checkpoints/textcnn/best_validation\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/textcnn/best_validation\n",
      "restored ./checkpoints/textcnn/best_validation\n",
      "\n",
      "\n",
      "\n",
      "Testing...\n",
      "\n",
      "\n",
      "Our classification model predict it to be: \n",
      "rock\n",
      "\n",
      "And the sentiment analysis result of the generated lyric is:\n",
      "The label is: neg\n",
      "The positive possibility is: 0.387181\n",
      "The negative possibility is: 0.612819\n",
      "The netural possibility is: 0.428872\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "run(\"rock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Rap lyric and verify\n",
    "We now use the 'rap' LSTM model to generate a rap lyric. And verify the result with our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n",
      "./checkpoints/rap/lyric-99\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/rap/lyric-99\n",
      "restored ./checkpoints/rap/lyric-99\n",
      "\n",
      "\n",
      "\n",
      "The generated lyrics: \n",
      "\n",
      "i got a couple chick every other , ghetto , i probably go still a \n",
      "\n",
      "a best of my lady but this real that come , back up my body \n",
      "oh , this yall dont really want no else baby \n",
      "i dont wan na see you and looking so sad boy you want to with your foot down \n",
      "tank out in your waist so you can seeing me and you smile and girl \n",
      "had to put a motherfucking girl and you know you selling good forever \n",
      "little boy , you got the house wrong it look at your friend ( you got dem back ) \n",
      "she dont wan na chill till i say ( uh , yeah , caked ) \n",
      "look at me now , \n",
      "on the table , yo , new , one of u , too late \n",
      "oh oh oh oh , oh , oh baby , oh let it be , \n",
      "baby you wan na be something else , yeah it love is right \n",
      "yeah , yall been here , right now , we wan na get more all right \n",
      "now it just like you , say that im love at the good night \n",
      "you know you where we say , ooh , ooh , ooh \n",
      "baby , i know you feel like ive been made for it , right right , right bro \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "./checkpoints/textcnn/best_validation\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/textcnn/best_validation\n",
      "restored ./checkpoints/textcnn/best_validation\n",
      "\n",
      "\n",
      "\n",
      "Testing...\n",
      "\n",
      "\n",
      "Our classification model predict it to be: \n",
      "rap\n",
      "\n",
      "And the sentiment analysis result of the generated lyric is:\n",
      "The label is: neg\n",
      "The positive possibility is: 0.425833\n",
      "The negative possibility is: 0.574167\n",
      "The netural possibility is: 0.169719\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "run(\"rap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
