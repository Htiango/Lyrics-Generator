{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15688 Project - Lyrics Generator & Classificator\n",
    "\n",
    "“Music can change the word because it can change people.” said by the Legendary U2 rocker Bono. A beautiful song usually has memorable lyrics that sometimes change people. However, it is not an easy task to write good lyrics. \n",
    "\n",
    "The aim of our project is to create a lyric generation model based on existing lyrics of different music genres - pop, rock, hip hop, etc - using machine learning algorithms that are common in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention:** <br>\n",
    "We develop this project via python scripts instead of Jupyter Notebook. <br>\n",
    "This tutorial mainly walks you through each step.  <br>\n",
    "It is highly recommended to go over this project via the command line we suggest instead of on this notebook. (In case some unexpected error.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Data Collection\n",
    "\n",
    "### Part 1. Singer and song collection\n",
    "In order to train the lyric model, the first step is to collect lyrics by genre. We collected the male and female artists' names from [music.163.com ](https://music.163.com/#/discover/artist/cat?id=2001)by copying the information on the webpage and saved them as csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting artists' names, we use the [musixmatch](http://api.musixmatch.com/ws/1.1/) api to collect the genre and name of songs of the artists. The results are exported as csv file so that we can count the most frequent genres among all the songs we collected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "15688 final project - lyric generator\n",
    "\n",
    "data collection\n",
    "\n",
    "retrieve the artists, genres and tracks and export to csv file\n",
    "\n",
    "API used: musixmatch Developer\n",
    "documentation: https://developer.musixmatch.com/documentation\n",
    "\n",
    "'''\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# load api key\n",
    "with open(\"../musicmatch_api.key\",'r') as f:\n",
    "    api = f.read()\n",
    "\n",
    "root = \"http://api.musixmatch.com/ws/1.1/\"\n",
    "\n",
    "def get_artist(api, pageNum, page_size=100, country = \"us\"):\n",
    "\n",
    "    '''\n",
    "    getting top artists and their genres\n",
    "    Args:\n",
    "        api: API key\n",
    "        pageNum: the page number for paginated results\n",
    "        page_size: the page size for paginated results. Range is 1 to 100\n",
    "        country: country of the artist ranking\n",
    "    Return:\n",
    "        df: a pandas dataframe containing artists, genres and genre id\n",
    "        all_genres: a set of all genres related to the artists found\n",
    "    '''\n",
    "    result = []\n",
    "    all_genres = set()\n",
    "    for i in range(pageNum):\n",
    "        param = {\n",
    "            \"apikey\":api,\n",
    "            \"country\": \"country\",\n",
    "            \"page\": i+1,\n",
    "            \"page_size\": page_size,\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "\n",
    "        singers = requests.get(root + \"chart.artists.get?\", params = param)\n",
    "        response = json.loads(singers.content)\n",
    "        artist_list = response.get(\"message\").get(\"body\").get(\"artist_list\")\n",
    "        \n",
    "        for artist in artist_list:\n",
    "            name = artist.get(\"artist\").get('artist_name')\n",
    "            genres = artist.get(\"artist\").get(\"primary_genres\").get(\"music_genre_list\")\n",
    "            for g in genres:\n",
    "                genre = g.get(\"music_genre\").get(\"music_genre_name\")\n",
    "                genre_id = g.get(\"music_genre\").get(\"music_genre_id\")\n",
    "                all_genres.add(genre)\n",
    "                result.append({\"artist\":name, \"genre\":genre, \"genre_id\":genre_id})\n",
    "    \n",
    "    df = pd.DataFrame(result)\n",
    "    df = df.loc[:, [\"artist\", \"genre\", \"genre_id\"]]\n",
    "    return df, all_genres\n",
    "\n",
    "\n",
    "def get_artist_genre(api, all_artist_list):\n",
    "\n",
    "    '''\n",
    "    getting the artists and their genres of given list\n",
    "\n",
    "    Args:\n",
    "        api: API key\n",
    "        all_artist_list: list of all the artists\n",
    "    Return:\n",
    "        df: a pandas dataframe containing artists, genres and genre id\n",
    "        all_genres: a set of all genres related to the artists found\n",
    "\n",
    "    '''\n",
    "    result = []\n",
    "    all_genres = set()\n",
    "    param = {\n",
    "            \"apikey\":api,\n",
    "            \"page\":1,\n",
    "            \"page_size\":10\n",
    "        }\n",
    "    for artist in all_artist_list:\n",
    "        param[\"q_artist\"] = artist\n",
    "\n",
    "        search_result = requests.get(root + \"artist.search?\", params = param)\n",
    "        response = json.loads(search_result.content)\n",
    "\n",
    "        artist_list = response.get(\"message\").get(\"body\").get(\"artist_list\")\n",
    "\n",
    "        if not artist_list:\n",
    "            continue\n",
    "        artist_item = artist_list[0]\n",
    "        \n",
    "        name = artist_item.get(\"artist\").get('artist_name')\n",
    "        genres = artist_item.get(\"artist\").get(\"primary_genres\").get(\"music_genre_list\")\n",
    "        for g in genres:\n",
    "            genre = g.get(\"music_genre\").get(\"music_genre_name\")\n",
    "            genre_id = g.get(\"music_genre\").get(\"music_genre_id\")\n",
    "            all_genres.add(genre)\n",
    "            result.append({\"artist\":name, \"genre\":genre, \"genre_id\":genre_id})\n",
    "    \n",
    "    df = pd.DataFrame(result)\n",
    "\n",
    "    if not df.empty:\n",
    "        df = df.loc[:, [\"artist\", \"genre\", \"genre_id\"]]\n",
    "    else:\n",
    "        print(\"result is an empty dataframe\")\n",
    "    return df, all_genres\n",
    "\n",
    "def get_songs(api, artist_df, page_size = 100):\n",
    "\n",
    "\n",
    "    '''\n",
    "    getting track names by artists and genre id\n",
    "\n",
    "    Args:\n",
    "        api: API key\n",
    "        artist_df: dataframe with columns of artist, genre and genre id\n",
    "        page_size: the page size for paginated results. Range is 1 to 100\n",
    "    Return:\n",
    "        df: a pandas dataframe containing artists, genres, genre id and the top\n",
    "        100 tracks with lyrics under that genre by the artist\n",
    "        \n",
    "    '''\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i, row in artist_df.iterrows(): \n",
    "        param = {\n",
    "                \"apikey\":api,\n",
    "                \"q_artist\": row['artist'],\n",
    "                \"f_music_genre_id\": row['genre_id'], # filter by genre id\n",
    "                \"f_has_lyrics\":\"True\", # only get tracks with lyrics\n",
    "                \"page\": 1,\n",
    "                \"page_size\": page_size\n",
    "            }\n",
    "\n",
    "        singer = requests.get(root + \"track.search?\", params = param)\n",
    "        response = json.loads(singer.content)\n",
    "        song_list = response.get(\"message\").get(\"body\").get(\"track_list\")\n",
    "        for song in song_list:\n",
    "    \n",
    "            track_name = song.get(\"track\").get(\"track_name\")\n",
    "            result.append(\n",
    "                {\n",
    "                \"artist\":row[\"artist\"], \n",
    "                \"genre\":row[\"genre\"], \n",
    "                \"genre_id\":row[\"genre_id\"],\n",
    "                \"track_name\":track_name\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    df = df.loc[:, [\"artist\", \"genre\",\"genre_id\", \"track_name\"]]\n",
    "    return df\n",
    "\n",
    "\n",
    "    #Step 1. get artists and their genres\n",
    "    # load the first 1,300 artists of from csv file\n",
    "    artist_df = pd.read_csv(\"./csv_files/all_female_artists.csv\", header = None)[:50]\n",
    "    artists_list = []\n",
    "    for col in artist_df.columns.values:\n",
    "        artists_list += list(artist_df[col])\n",
    "\n",
    "    artist_genre_df, all_genres = get_artist_genre(api, artists_list)\n",
    "    artist_genre_df.to_csv(\"./csv_files/all_female_artist_genre.csv\",index = False)\n",
    "\n",
    "    #Step 2. get songs by artists and genres\n",
    "    artist_df = pd.read_csv(\"./csv_files/all_female_artist_genre.csv\")[:1000]\n",
    "    print(artist_df.shape)\n",
    "    song_df = get_songs(api, artist_df)\n",
    "    song_df.to_csv(\"./csv_files/all_female_artist_genre_track.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Lyrics collection via *lyricwikia*\n",
    "\n",
    "With all the song names, we use [lyricwikia](https://github.com/enricobacis/lyricwikia) package in Python to collect the lyrics. The package can be installed with pip.\n",
    "\n",
    "```python\n",
    "pip3 install lyricwikia\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricwikia as ly\n",
    "\n",
    "#request lyric song by song\n",
    "#row by row in the dataframe\n",
    "def getLyrics(songs):\n",
    "    i = -1\n",
    "    print(\"Total songs number:\" + str(songs.shape[0]))\n",
    "    for index, row in songs.iterrows():\n",
    "        i += 1\n",
    "        if i%100 == 0:\n",
    "            print(\"Processing song [\" + str(i) + \"]\")\n",
    "\n",
    "        song = row['track_name']\n",
    "        #print(song)\n",
    "        artist = row['artist']\n",
    "        try:\n",
    "            lyric = ly.get_lyrics(artist, song, linesep='\\n', timeout=None)\n",
    "            songs.loc[index,'lyric'] = lyric\n",
    "        except:\n",
    "            continue    \n",
    "        #print(lyric)\n",
    "    return songs\n",
    "\n",
    "\n",
    "def run(oriFile, newFile):\n",
    "    songs = pd.read_csv(oriFile, encoding = \"ISO-8859-1\")\n",
    "    songs = getLyrics(songs)\n",
    "    songs = songs.dropna()\n",
    "    #print(songs)\n",
    "    songs.to_csv(newFile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run in command line:\n",
    "\n",
    "```bash\n",
    "python3 get_lyrics.py -h\n",
    "```\n",
    "to choose the csv file of the lyric track and customize the output path of the lyric file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the csv file of songs and their genres, we found the top 3 genres are:\n",
    "\n",
    "* Pop\n",
    "* Hip Hop/Rap\n",
    "* Rock\n",
    "\n",
    "We will train the model based on these three genres. Therefore, we will extract and generate the dataset of lyrics of each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_lyrics(csv_path):    \n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df = df.iloc[:,1:]\n",
    "\n",
    "    result = []\n",
    "    for genre in ['Pop','Rock','Hip Hop/Rap']:\n",
    "        result.append(df[df['genre'] == genre])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_female = split_lyrics('../csv_files/all_female_artist_lyrics.csv')\n",
    "    df_male = split_lyrics('../csv_files/all_male_artist_lyrics.csv')\n",
    "\n",
    "    for d, f in zip(df_female,df_male):\n",
    "        genre = d.iloc[0,1]\n",
    "        genre = genre.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "        df = pd.concat([d,f])\n",
    "        df.to_csv('../csv_files/lyrics_' + genre +\".csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train the model with 3 datasets consisting of lyrics of different genres. \n",
    "\n",
    "And the number of lyrics in each generes are:\n",
    "\n",
    "| genre | lyric number |\n",
    "|---|---|\n",
    "| rap | 5039 |\n",
    "| pop | 21998 |\n",
    "| rock | 6503 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The singer, song and lyric files are all stored in `../csv_files/` directory*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we'll apply two different deep learning methods:\n",
    "+ LSTM model to generate chosen genre of lyrics\n",
    "+ CNN model to classify a lyric into specific genre\n",
    "\n",
    "Since these two methods need different preprocessing, here we divide data preprocessing part into 2 part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Preprocessing for LSTM model:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LSTM model, it is important to know the start and the end of a sentense. So here in the preprocessing stage, we manually add a start mark and an end mark to each lyric. And then use `nltk` package to tokenize lyrics into words and stem them. Then remove all the rare words.   <br>\n",
    "The most important method below is the `process` method, which generates all the features needed by the LSTM model. The returning X represents the word id sequences in each batch size lyrics. The Y is almost the same as X, except it is actually X moving 1 word to the right. And we also need a word to Id dict so that we can transform generated ids into words in the test stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "START_MARK = \"[\"\n",
    "END_MARK = \"]\"\n",
    "\n",
    "def seperate(docs_ls, is_rnn):\n",
    "    if is_rnn:\n",
    "        docs_raw = [tokenize(START_MARK+str(doc)+END_MARK) for doc in docs_ls]\n",
    "    else:\n",
    "        docs_raw = [tokenize(str(doc)) for doc in docs_ls]\n",
    "    docs = remove_stopwords(docs_raw)\n",
    "    print(\" \".join(docs[0]))\n",
    "    return docs\n",
    "\n",
    "def remove_stopwords(docs):\n",
    "    stopwords = get_rare_words(docs)\n",
    "    stopwords = set(stopwords)\n",
    "    res = [[word for word in doc if word not in stopwords ] for doc in docs]\n",
    "    return res\n",
    "\n",
    "\n",
    "def tokenize(text, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    Inputs:\n",
    "        text: str: raw text\n",
    "        lemmatizer: an instance of a class implementing the lemmatize() method\n",
    "                    (the default argument is of type nltk.stem.wordnet.WordNetLemmatizer)\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    text = text.replace(\"\\n\", \".\\n\")\n",
    "    text = text.replace(\"\\t\", \" \")\n",
    "    \n",
    "    punc = string.punctuation\n",
    "    for c in punc:\n",
    "        if c in text:\n",
    "            text = text.replace(c, ' '+c+' ')\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    res = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            word = lemmatizer.lemmatize(token)\n",
    "            res.append(str(word))\n",
    "        except:\n",
    "            continue\n",
    "    docs=nltk.word_tokenize(\" \".join(res))\n",
    "    return res\n",
    "\n",
    "def get_rare_words(tokens_ls):\n",
    "    \"\"\" use the word count information across all tweets in training data to come up with a feature list\n",
    "    Inputs:\n",
    "        processed_tweets: pd.DataFrame: the output of process_all() function\n",
    "    Outputs:\n",
    "        list(str): list of rare words, sorted alphabetically.\n",
    "    \"\"\"\n",
    "    counter = Counter([])\n",
    "    for tokens in tokens_ls:\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    rare_tokes = [k for k,v in counter.items() if v<=3]\n",
    "    rare_tokes.sort()\n",
    "    return rare_tokes\n",
    "\n",
    "def process(lyrics, batchSize=10, is_rnn=True):\n",
    "    \"\"\"\n",
    "    It will change lyrics to vetors as well as build the\n",
    "    features and labels for LSTM\n",
    "\n",
    "    lyric: list of str. all of the lyrics\n",
    "    return: (X, Y, vocab_size, vocab_ID, vocab)\n",
    "    \"\"\"\n",
    "\n",
    "    lyricDocs = seperate(lyrics, is_rnn)\n",
    "    print(\"Totally %d lyrics.\"%len(lyricDocs))\n",
    "\n",
    "    allWords = {}\n",
    "    for lyricDoc in lyricDocs:\n",
    "        for word in lyricDoc:\n",
    "            if word not in allWords:\n",
    "                allWords[word] = 1\n",
    "            else:\n",
    "                allWords[word] += 1\n",
    "\n",
    "    wordPairs = sorted(allWords.items(), key = lambda x: -x[1])\n",
    "    words, a= zip(*wordPairs)\n",
    "    words += (\" \", )\n",
    "    wordToID = dict(zip(words, range(len(words)))) #word to ID\n",
    "    wordTOIDFun = lambda A: wordToID.get(A, len(words))\n",
    "\n",
    "    lyricVector = [([wordTOIDFun(word) for word in lyricDoc]) for lyricDoc in lyricDocs] \n",
    "\n",
    "    batchNum = (len(lyrics) - 1) // batchSize \n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(batchNum):\n",
    "        batchVec = lyricVector[i*batchSize: (i+1)*batchSize]\n",
    "\n",
    "        maxLen = max([len(vector) for vector in batchVec])\n",
    "\n",
    "        temp = np.full((batchSize, maxLen), wordTOIDFun(\" \"),np.int32)\n",
    "\n",
    "        for j in range(batchSize):\n",
    "            temp[j, :len(batchVec[j])] = batchVec[j]\n",
    "\n",
    "        X.append(temp)\n",
    "\n",
    "        temp_copy = np.copy(temp)\n",
    "        temp_copy[:, :-1] = temp[:, 1:]\n",
    "        Y.append(temp_copy)\n",
    "    return X, Y, len(words) + 1, wordToID, words\n",
    "\n",
    "\n",
    "def generate_feature(filename, ouput_path):\n",
    "    \"\"\"\n",
    "    This methods is mainly for printing out the result to examine\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    docs = df['lyric'].values.tolist()[:100]\n",
    "    print(docs[0])\n",
    "    print()\n",
    "    print()\n",
    "    X, Y, size, wordToId, words = process(docs)\n",
    "    print(size)\n",
    "    print(X[0][0].shape)\n",
    "\n",
    "def pretreatment(filename, batchSize):\n",
    "    df = pd.read_csv(filename)\n",
    "    docs = df['lyric'].values\n",
    "    P = np.random.permutation(len(docs))\n",
    "    print(\"Shuffling\")\n",
    "    docs = docs[P].tolist()\n",
    "    print(\"Processing\")\n",
    "    return process(docs, batchSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the processing is a little bit time-consuming, we save the result into a pickle file to make it easier for us to testing LSTM model. We use the following code to save preprocessed LSTM data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(input_path, param_saving_path, batch_size):\n",
    "    X, Y, wordNum, wordToID, words = lyric_processing.pretreatment(input_path,batch_size)\n",
    "    data = {'X': X, \"Y\":Y, \"wordNum\":wordNum, \n",
    "        \"wordToID\": wordToID, \"words\":words, 'batch_size':batch_size}\n",
    "\n",
    "    with open(param_saving_path, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run in command line\n",
    "```bash\n",
    "python3 save_data.py -h\n",
    "```\n",
    "to choose the input raw data and customize your output file name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Preprocessing for CNN model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the LSTM model, the start and the end is not important in CNN model. <br>\n",
    "We use the same method in Part 1. to tokenize lyrics and generate a vocabulary. And then we pad each lyrics into the longest (or our chosen) length. (Here we pad using the mark `<PAD>`) Then we save parameters into a pickle file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lyric_processing import tokenize, remove_stopwords, seperate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.keras as kr\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "filename_rock = \"../csv_files/lyrics_Rock.csv\"\n",
    "filename_pop  = \"../csv_files/lyrics_Pop.csv\"\n",
    "filename_rap  = \"../csv_files/lyrics_Hip_Hop_Rap.csv\"\n",
    "doc_num = 5000\n",
    "max_length = 800\n",
    "categories = ['pop','rock', 'rap']\n",
    "cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "\n",
    "\n",
    "def load_lyrics(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    docs = df['lyric'].values\n",
    "    return docs\n",
    "\n",
    "def get_raw_data():\n",
    "    # --------------- load and select data -------------\n",
    "    lyric_rock = load_lyrics(filename_rock)\n",
    "    lyric_pop = load_lyrics(filename_pop)\n",
    "    lyric_rap = load_lyrics(filename_rap)\n",
    "    P_rap = np.random.permutation(lyric_rap.shape[0])[:doc_num]\n",
    "    P_rock = np.random.permutation(lyric_rock.shape[0])[:doc_num]\n",
    "    P_pop = np.random.permutation(lyric_pop.shape[0])[:doc_num]\n",
    "\n",
    "    lyric_pop_chosen = lyric_pop[P_pop]\n",
    "    lyric_rap_chosen = lyric_rap[P_rap]\n",
    "    lyric_rock_chosen = lyric_rock[P_rock]\n",
    "    lyrics = np.concatenate((lyric_pop_chosen, lyric_rock_chosen, lyric_rap_chosen))\n",
    "\n",
    "    y_pop = np.array([cat_to_id['pop'] for _ in lyric_pop_chosen])\n",
    "    y_rock = np.array([cat_to_id['rock'] for _ in lyric_rock_chosen])\n",
    "    y_rap = np.array([cat_to_id['rap'] for _ in lyric_rap_chosen])\n",
    "    y = np.concatenate((y_pop, y_rock, y_rap))\n",
    "\n",
    "    return lyrics, y\n",
    "\n",
    "\n",
    "def process(param_saving_path):\n",
    "    lyrics, y = get_raw_data()\n",
    "    lyricDocs = seperate(lyrics, False)\n",
    "    print(\"Totally %d lyrics.\"%len(lyricDocs))\n",
    "    allWords = {}\n",
    "    for lyricDoc in lyricDocs:\n",
    "        for word in lyricDoc:\n",
    "            if word not in allWords:\n",
    "                allWords[word] = 1\n",
    "            else:\n",
    "                allWords[word] += 1\n",
    "\n",
    "    wordPairs = sorted(allWords.items(), key = lambda x: -x[1])\n",
    "    words, a= zip(*wordPairs)\n",
    "    words += (\" \", )\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    wordToID = dict(zip(words, range(len(words)))) #word to ID\n",
    "    wordTOIDFun = lambda A: wordToID.get(A, len(words))\n",
    "\n",
    "    lyricVector = [([wordTOIDFun(word) for word in lyricDoc]) for lyricDoc in lyricDocs]\n",
    "\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(lyricVector, max_length)\n",
    "    y_pad = kr.utils.to_categorical(y, num_classes=len(cat_to_id))\n",
    "\n",
    "    data = {'X': x_pad, 'Y': y_pad, 'wordToID': wordToID, 'seq_length': max_length}\n",
    "    \n",
    "    with open(param_saving_path, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print('Finish!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run in command line\n",
    "```bash\n",
    "python3 classification_preprocess.py -h\n",
    "```\n",
    "to customize your output parameter pickle file name and path. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Training Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. LSTM model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use LSTM model to generate lyrics for different genres. [This blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) clearly states the knowledge of LSTM. <br>\n",
    "For each genres, we do preprocessing and save the needed parameters into pickle files. Then we load the specific pickle file and train the LSTM model for the genre. <br>\n",
    "The word IDs will be embedded into a dense representation before feeding to the LSTM, which is called embedding layer. Here we use 2 layers of LSTM to process the data, followed by softmax representing each word's appearing probability. <br>\n",
    "In the training stage, we do 100 epoches. Since the data we use is very large and LSTM model is very slow to train. Here we use AWS to train 3 models for 3 genres. Even on AWS GPU server, it took nearly 40 hours to train. (40+ hours to train for pop, 18+ hours to train for rap and 10+ hours to train for rock). <br>\n",
    "The models are saved in:\n",
    "+ rap model:  `./checkpoints/rap/` \n",
    "+ rock model: `./checkpoints/rock/`\n",
    "+ pop model:  `./checkpoints/pop/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# batchSize = 10\n",
    "learningRateBase = 0.001\n",
    "learningRateDecreaseStep = 100\n",
    "epochNum = 100                    # train epoch\n",
    "\n",
    "generateNum = 1\n",
    "\n",
    "checkpointsPath = \"./checkpoints\" # checkpoints location\n",
    "\n",
    "def buildModel(wordNum, gtX, hidden_units = 128, layers = 2):\n",
    "    \"\"\"build rnn\"\"\"\n",
    "    with tf.variable_scope(\"embedding\"): #embedding\n",
    "        embedding = tf.get_variable(\"embedding\", [wordNum, hidden_units], dtype = tf.float32)\n",
    "        inputbatch = tf.nn.embedding_lookup(embedding, gtX)\n",
    "\n",
    "    basicCell = tf.contrib.rnn.BasicLSTMCell(hidden_units)    \n",
    "    stackCell = tf.contrib.rnn.MultiRNNCell([basicCell] * layers)\n",
    "    initState = stackCell.zero_state(np.shape(gtX)[0], tf.float32)\n",
    "    outputs, finalState = tf.nn.dynamic_rnn(stackCell, inputbatch, initial_state = initState)\n",
    "    outputs = tf.reshape(outputs, [-1, hidden_units])\n",
    "\n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        w = tf.get_variable(\"w\", [hidden_units, wordNum])\n",
    "        b = tf.get_variable(\"b\", [wordNum])\n",
    "        logits = tf.matmul(outputs, w) + b\n",
    "\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    return logits, probs, stackCell, initState, finalState\n",
    "\n",
    "def train(X, Y, wordNum, batchSize,reload=True):\n",
    "    \"\"\"train model\"\"\"\n",
    "    gtX = tf.placeholder(tf.int32, shape=[batchSize, None])  # input\n",
    "    gtY = tf.placeholder(tf.int32, shape=[batchSize, None])  # output\n",
    "    logits, probs, a, b, c = buildModel(wordNum, gtX)\n",
    "    targets = tf.reshape(gtY, [-1])\n",
    "    #loss\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [targets],\n",
    "                                                              [tf.ones_like(targets, dtype=tf.float32)], wordNum)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, a = tf.clip_by_global_norm(tf.gradients(cost, tvars), 5)\n",
    "    learningRate = learningRateBase\n",
    "    optimizer = tf.train.AdamOptimizer(learningRate)\n",
    "    trainOP = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    globalStep = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        if reload:\n",
    "            checkPoint = tf.train.get_checkpoint_state(checkpointsPath)\n",
    "            # if have checkPoint, restore checkPoint\n",
    "            if checkPoint and checkPoint.model_checkpoint_path:\n",
    "                saver.restore(sess, checkPoint.model_checkpoint_path)\n",
    "                print(\"restored %s\" % checkPoint.model_checkpoint_path)\n",
    "            else:\n",
    "                print(\"no checkpoint found!\")\n",
    "\n",
    "        for epoch in range(epochNum):\n",
    "            if globalStep % learningRateDecreaseStep == 0: #learning rate decrease by epoch\n",
    "                learningRate = learningRateBase * (0.95 ** epoch)\n",
    "            epochSteps = len(X) # equal to batch\n",
    "            for step, (x, y) in enumerate(zip(X, Y)):\n",
    "                globalStep = epoch * epochSteps + step\n",
    "                a, loss = sess.run([trainOP, cost], feed_dict = {gtX:x, gtY:y})\n",
    "                print(\"epoch: %d steps:%d/%d loss:%3f\" % (epoch,step,epochSteps,loss))\n",
    "                if globalStep%1000==0:\n",
    "                    print(\"save model\")\n",
    "                    # save_path = saver.save(sess, '/output/model.ckpt')\n",
    "                    save_path = saver.save(sess,checkpointsPath + \"/lyric\",global_step=epoch)\n",
    "                    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "def probsToWord(weights, words):\n",
    "    \"\"\"probs to word\"\"\"\n",
    "    t = np.cumsum(weights) #prefix sum\n",
    "    s = np.sum(weights)\n",
    "    coff = np.random.rand(1)\n",
    "    index = int(np.searchsorted(t, coff * s)) # large margin has high possibility to be sampled\n",
    "    return words[index]\n",
    "\n",
    "def test(wordNum, wordToID, words, model_path=checkpointsPath):\n",
    "    \"\"\"generate lyric\"\"\"\n",
    "    gtX = tf.placeholder(tf.int32, shape=[1, None])  # input\n",
    "    logits, probs, stackCell, initState, finalState = buildModel(wordNum, gtX)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        checkPoint = tf.train.get_checkpoint_state(model_path)\n",
    "        # if have checkPoint, restore checkPoint\n",
    "        if checkPoint and checkPoint.model_checkpoint_path:\n",
    "            print(checkPoint.model_checkpoint_path)\n",
    "            saver.restore(sess, checkPoint.model_checkpoint_path)\n",
    "            print(\"restored %s\" % checkPoint.model_checkpoint_path)\n",
    "            print(\"\\n\\n\")\n",
    "        else:\n",
    "            print(\"no checkpoint found!\")\n",
    "            exit(0)\n",
    "\n",
    "        lyrics = []\n",
    "        for i in range(generateNum):\n",
    "            state = sess.run(stackCell.zero_state(1, tf.float32))\n",
    "            x = np.array([[wordToID['[']]]) # init start sign\n",
    "            probs1, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n",
    "            word = probsToWord(probs1, words)\n",
    "            lyric = ''\n",
    "            while word != ']' and word != ' ':\n",
    "                if word == '.':\n",
    "                    try:\n",
    "                        if not (lyric[-1]=='.' and lyric[-2] == '.'):\n",
    "                            lyric += '. '\n",
    "                    except:\n",
    "                        pass\n",
    "                else:\n",
    "                    lyric += word + ' '\n",
    "                x = np.array([[wordToID[word]]])\n",
    "                #print(word)\n",
    "                probs2, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n",
    "                word = probsToWord(probs2, words)\n",
    "            print(\"The generated lyrics: \\n\")\n",
    "            print(lyric.replace(\". \", \"\\n\"))\n",
    "            lyrics.append(lyric)\n",
    "        return lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run in command line\n",
    "```bash\n",
    "python3 main.py -h\n",
    "```\n",
    "to choose training or testing the LSTM mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the method mentioned in [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882) <br>\n",
    "The architecture of the model is listed as below, which is taken from the above article. \n",
    "![CNN model](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Example-of-a-CNN-Filter-and-Polling-Architecture-for-Natural-Language-Processing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I set the word embedding dimension to be 600 and each sequence length to be 800. (If not satisfied we add `<PAD>` in the front. ) We choose 256 convolution filters and each size is 5 followed by a max-over-time polling. Then we use a fully connected layers with drop out and ReLU. And finally use softmax to do the classification. (Here we do 3-class classification: the 3 genres mentioned above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import tensorflow.contrib.keras as kr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import argparse\n",
    "from classification_preprocess import cat_to_id\n",
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "\n",
    "\n",
    "save_dir = './checkpoints/textcnn'\n",
    "save_path = os.path.join(save_dir, 'best_validation')\n",
    "param_saving_path = '../data/param-classify.dat'\n",
    "tensorboard_dir = './tensorboard/textcnn'\n",
    "validation_rate = 0.1\n",
    "\n",
    "class TCNNConfig(object):\n",
    "    \"\"\"CNN param\"\"\"\n",
    "    embedding_dim = 64  # word vector dimension\n",
    "    seq_length = 800  # sequense length\n",
    "    num_classes = 3  # class number\n",
    "    num_filters = 256  # kernel number\n",
    "    kernel_size = 5  # kernel size\n",
    "    vocab_size = 5000  # vocab size\n",
    "\n",
    "    hidden_dim = 128  # fully connected neuro number\n",
    "\n",
    "    dropout_keep_prob = 0.5  # dropout keeping rate\n",
    "    learning_rate = 1e-3  # learning rate\n",
    "\n",
    "    batch_size = 64  # batch size\n",
    "    num_epochs = 10  # total epoch number\n",
    "\n",
    "    print_per_batch = 10  # output iterations\n",
    "    save_per_batch = 10  # save tensorboard iterations\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"text classification，CNN model\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        self.cnn()\n",
    "\n",
    "    def cnn(self):\n",
    "        \"\"\"CNN model\"\"\"\n",
    "        # word embedding\n",
    "        with tf.device('/cpu:0'):\n",
    "            embedding = tf.get_variable('embedding', [self.config.vocab_size, self.config.embedding_dim])\n",
    "            embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n",
    "\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            # CNN layer\n",
    "            conv = tf.layers.conv1d(embedding_inputs, self.config.num_filters, self.config.kernel_size, name='conv')\n",
    "            # global max pooling layer\n",
    "            gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')\n",
    "\n",
    "        with tf.name_scope(\"score\"):\n",
    "            # fully connected layer，with dropout and ReLU\n",
    "            fc = tf.layers.dense(gmp, self.config.hidden_dim, name='fc1')\n",
    "            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "            fc = tf.nn.relu(fc)\n",
    "\n",
    "            # classifier\n",
    "            self.logits = tf.layers.dense(fc, self.config.num_classes, name='fc2')\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # predictor\n",
    "\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            # loss function，cross entropy\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            # optimizor\n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # accuracy\n",
    "            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"get time\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "def batch_iter(x, y, batch_size=64):\n",
    "    \"\"\"generate batchsize data\"\"\"\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "\n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indices]\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n",
    "\n",
    "def feed_data(model, x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def evaluate(model, sess, x_, y_):\n",
    "    \"\"\"evaluate the loss and accuracy\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(model, x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "def train(filename):\n",
    "    config = TCNNConfig()\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    x = data['X']\n",
    "    y = data['Y']\n",
    "    print(len(x))\n",
    "    P = np.random.permutation(len(x))\n",
    "    x = x[P]\n",
    "    y = y[P]\n",
    "\n",
    "    wordToID = data['wordToID']\n",
    "    seq_length = data['seq_length']\n",
    "    config.vocab_size = len(wordToID)\n",
    "    config.seq_length = seq_length\n",
    "\n",
    "    model = TextCNN(config)\n",
    "\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "\n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "    \n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    idx = int(x.shape[0] * validation_rate)\n",
    "    x_train = x[idx:]\n",
    "    x_val = x[:idx]\n",
    "    y_train = y[idx:]\n",
    "    y_val = y[:idx]\n",
    "    \n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer.add_graph(session.graph)\n",
    "    \n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # total batch number\n",
    "    best_acc_val = 0.0  # best validation accuracy\n",
    "    last_improved = 0  # last improving\n",
    "    require_improvement = 1000  # if not improving after 1000 iterations, end early\n",
    "    \n",
    "    flag = False\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            feed_dict = feed_data(model, x_batch, y_batch, config.dropout_keep_prob)\n",
    "            \n",
    "            if total_batch % config.save_per_batch == 0:\n",
    "                # save to tensorboard scalar\n",
    "                s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                writer.add_summary(s, total_batch)\n",
    "\n",
    "\n",
    "            if total_batch % config.print_per_batch == 0:\n",
    "                # get the loss and accuracy on training set and validation set\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                loss_val, acc_val = evaluate(model, session, x_val, y_val)  # todo\n",
    "\n",
    "                if acc_val > best_acc_val:\n",
    "                    # save the best result\n",
    "                    best_acc_val = acc_val\n",
    "                    last_improved = total_batch\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "                    print(\"Save model!\")\n",
    "                    improved_str = '*'\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>4.4}, Train Acc: {2:>5.2%},' \\\n",
    "                      + ' Val Loss: {3:>4.4}, Val Acc: {4:>5.2%}, Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n",
    "\n",
    "            session.run(model.optim, feed_dict=feed_dict)  \n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # early end\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break \n",
    "        if flag:\n",
    "            break\n",
    "\n",
    "\n",
    "def test(text, filename, genre, model_path=save_dir):\n",
    "\n",
    "    config = TCNNConfig()\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    wordToID = data['wordToID']\n",
    "    seq_length = data['seq_length']\n",
    "    config.vocab_size = len(wordToID)\n",
    "    config.seq_length = seq_length\n",
    "\n",
    "    model = TextCNN(config)\n",
    "\n",
    "    text_ids = [[wordToID[word] for word in text.split(\" \") if word in wordToID]]\n",
    "    # print(text_ids)\n",
    "    y = np.array([cat_to_id[genre]])\n",
    "\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(text_ids, seq_length)\n",
    "    y_pad = kr.utils.to_categorical(y, num_classes=len(cat_to_id)) \n",
    "\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        checkPoint = tf.train.get_checkpoint_state(model_path)\n",
    "        # if have checkPoint, restore checkPoint\n",
    "        if checkPoint and checkPoint.model_checkpoint_path:\n",
    "            print(checkPoint.model_checkpoint_path)\n",
    "            saver.restore(session, checkPoint.model_checkpoint_path)\n",
    "            print(\"restored %s\" % checkPoint.model_checkpoint_path)\n",
    "            print(\"\\n\\n\")\n",
    "        else:\n",
    "            print(\"no checkpoint found!\")\n",
    "            exit(0)\n",
    "\n",
    "        \n",
    "\n",
    "        print('Testing...')\n",
    "\n",
    "        feed_dict = feed_data(model, x_pad, y_pad, 1.0)\n",
    "        y_pred = session.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "        return list(cat_to_id)[y_pred[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training processing is in `classification.ipynb`, where prints out the details in training. <br>\n",
    "You can also run in command line\n",
    "```bash\n",
    "python3 classification_model.py -h\n",
    "```\n",
    "to choose training or testing mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loss figure is:\n",
    "![Screen Shot 2018-05-07 at 9.25.54 PM](https://oh1ulkf4j.qnssl.com/Screen Shot 2018-05-07 at 9.25.54 PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the training accuracy figure is:\n",
    "![Screen Shot 2018-05-07 at 9.25.40 PM](https://oh1ulkf4j.qnssl.com/Screen Shot 2018-05-07 at 9.25.40 PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figures are recorded by tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saving model's accuracy on testing dataset is 77.40%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is saved in `./checkpoints/textcnn/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 4. Display Result (It's show time!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use AWS server to train 3 LSTM lyric generator models for 3 genres and train the CNN classification model locally. With those saving models, now we can use our LSTM model to generate lyric in chosen genre. And then use our CNN classification model to test the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get lyric in random, instead of selecting the word with the highest probability, I map the probability to an interval and randomly sample one. See in `probsToWord` method in Step3 part1. (Of course each lyric starts with the starting mark `[`) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are recommended to run in command line:\n",
    "```bash\n",
    "python3 generator.py -g [pop/rock/rap]\n",
    "```\n",
    "to generate a chosen genre lyric and verify in our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from model import test as generate_model\n",
    "from classification_model import test as classify_model\n",
    "import tensorflow as tf\n",
    "\n",
    "pop_model = \"./checkpoints/pop\"\n",
    "pop_save = \"./generate-param/param-pop-10-test.dat\"\n",
    "\n",
    "rock_model = \"./checkpoints/rock\"\n",
    "rock_save = \"./generate-param/param-rock-10-test.dat\"\n",
    "\n",
    "rap_model = \"./checkpoints/rap\"\n",
    "rap_save = \"./generate-param/param-rap-10-test.dat\"\n",
    "\n",
    "classify_model_path = \"./checkpoints/textcnn\"\n",
    "classify_save = \"./generate-param/param-classify-test.dat\"\n",
    "\n",
    "\n",
    "def run(genre):\n",
    "    if genre == 'pop':\n",
    "        model_path = pop_model\n",
    "        data_path = pop_save\n",
    "    elif genre == 'rock':\n",
    "        model_path = rock_model\n",
    "        data_path = rock_save\n",
    "    elif genre == 'rap':\n",
    "        model_path = rap_model\n",
    "        data_path = rap_save\n",
    "    else:\n",
    "        print(\"Unexpected input!\")\n",
    "\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print('generating...')\n",
    "\n",
    "    lyrics = generate_model(data['wordNum'], \n",
    "        data['wordToID'], \n",
    "        data['words'], \n",
    "        model_path=model_path)\n",
    "\n",
    "    print('\\n\\n')\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    predicted = classify_model(lyrics[0], classify_save, genre, model_path=classify_model_path)\n",
    "    print(\"\\n\\nOur classification model predict it to be: \")\n",
    "    print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Pop lyric and verify\n",
    "We now use the 'Pop' LSTM model to generate a pop lyric. And verify the result with our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n",
      "./checkpoints/pop/lyric-99\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/pop/lyric-99\n",
      "restored ./checkpoints/pop/lyric-99\n",
      "\n",
      "\n",
      "\n",
      "The generated lyrics: \n",
      "\n",
      "sometimes i wake up in you \n",
      "even even though i feel mean it \n",
      "hell give it up just for you \n",
      "\n",
      "do you wan na play a thing that you ever wan na do \n",
      "i really wan na keep comin over the wire \n",
      "you wild at me and this is beat everyday \n",
      "put your down game take your breath \n",
      "push walk away from my lying \n",
      "walk em take it , yeah \n",
      "and watch ( hey boy ) \n",
      "cause you know it dont matter anyway \n",
      "we the touch our worst people \n",
      "but it the time ( all right ) \n",
      "do they know ? ( tell me ) \n",
      "( yeah yeah ) \n",
      "\n",
      "you know im whole word \n",
      "all you want your world to go \n",
      "all my love and my world will see me \n",
      "baby , i wan na get i \n",
      "see there something about my lovin \n",
      "and when it doesnt sound \n",
      "yes i really wan na \n",
      "just like i never knew \n",
      "that ill be real you \n",
      "baby baby cmon \n",
      "i need a good little girl \n",
      "\n",
      "get your kick back what you want to do \n",
      "let get over so much for you \n",
      "jump with that clothes on before \n",
      "i want to shake the ground in the middle of your love \n",
      "\n",
      "im just like a movie \n",
      "girl that you need a thing to blame with your skin \n",
      "aint that electricity \n",
      "it somethin bout you cant change \n",
      "when the light break me down \n",
      "the heat dont get your sexy face \n",
      "stumbling around and a full waitress \n",
      "moving around while im on home \n",
      "well life is hard to get real \n",
      "youll be okay if youre right \n",
      "\n",
      "are you around me or all that i feel \n",
      "when all im saying it wasnt really something about ya ya \n",
      "n i want to say crack is my secret \n",
      "youre sorry or do you think im crazy but you already figured a ? \n",
      "were out of two skin on the sand \n",
      "youre like throwing into that lately \n",
      "im not too cool about the thing you are \n",
      "im in your heart with me \n",
      "got somethin to say to you \n",
      "no club gon na get it \n",
      "get a fix \n",
      "im telling you \n",
      "that i get down just a day for a problem \n",
      "ima go away with you i cant save a fool \n",
      "you know \n",
      "thing i want when you should have changed \n",
      "\n",
      "i want to be your man \n",
      "but youre freaking me \n",
      "you say im okay \n",
      "boy youre still me \n",
      "i wan na have to get away from you \n",
      "ready to give you all \n",
      "it my too right to get chill \n",
      "and i dont know why \n",
      "i thought your kiss ha got to hide \n",
      "a new romance can get to \n",
      "your pride \n",
      "i only want to be so pretty \n",
      "tonight \n",
      "you need a second impression \n",
      "do i just stop the next to you \n",
      "come a - show what ive got ta get crazy \n",
      "baby , i wan na need you baby \n",
      "just wan na lose your love \n",
      "because you got what you need to do , do it right \n",
      "i just wan na tell you make my heart beat baby \n",
      "so baby , ( baby ) \n",
      "\n",
      "( hey ) \n",
      "hello \n",
      "hello \n",
      "whatcha gon na do ? \n",
      "im not gon na run ill ( oh baby ) \n",
      "( mmm yeah baby ) \n",
      "( yeah yeah yeah ) \n",
      "i know you want to take control ( yeah ) \n",
      "how we got ta do into you ( i can baby you know ) \n",
      "tell you you can just get a little fun \n",
      "\n",
      "( baby \n",
      "\n",
      ") \n",
      "yeah , babe \n",
      "\n",
      "\n",
      "\n",
      "./checkpoints/textcnn/best_validation\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/textcnn/best_validation\n",
      "restored ./checkpoints/textcnn/best_validation\n",
      "\n",
      "\n",
      "\n",
      "Testing...\n",
      "\n",
      "\n",
      "Our classification model predict it to be: \n",
      "pop\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "run(\"pop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a rock lyric and verify\n",
    "We now use the 'rock' LSTM model to generate a rock lyric. And verify the result with our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n",
      "./checkpoints/rock/lyric-98\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/rock/lyric-98\n",
      "restored ./checkpoints/rock/lyric-98\n",
      "\n",
      "\n",
      "\n",
      "The generated lyrics: \n",
      "\n",
      "shed the lover and the sun \n",
      "when i see you walking down \n",
      "from the window light \n",
      "you dont remember my mind \n",
      "my life ha have the holy reason \n",
      "\n",
      "all the different thing im soul \n",
      "bite em waving for myself \n",
      "\n",
      "i have lived the hour \n",
      "but it we saved wound \n",
      "taking thousand year in \n",
      "their house are getting chair \n",
      "at the bottom of the \n",
      "and in their street \n",
      "year got a wisdom \n",
      "and the hangman move his head \n",
      "and im his wicked tongue \n",
      "step aside , you did your model and \n",
      "he wa saying \n",
      "i like them we cant meet \n",
      "\n",
      "seemed my time to you , every day \n",
      "but once i wa hoping there wa here to end \n",
      "and you can trust to get away \n",
      "\n",
      "same before his run across blind \n",
      "and i wa my strength is with these \n",
      "existence will always be opened \n",
      "what is much better \n",
      "cause just so good when it seems \n",
      "\n",
      "im tired of working \n",
      "when your window fade \n",
      "im livin in paradise \n",
      "\n",
      "ive heard the writing on the wall \n",
      "\n",
      "crazy can i count here with me \n",
      "i dont want to think of love \n",
      "for the hidden say that is true or \n",
      "\n",
      "the truth is hard to hurt \n",
      "before you meet me more \n",
      "\n",
      "i and what the truth will someone \n",
      "\n",
      "i will tell you , i bleed to feed this \n",
      "and that it changed \n",
      "i dont need you \n",
      "i dont have to happen \n",
      "i ever seem and you said why \n",
      "i just got the case you set my cry \n",
      "shake my heart , you meditate \n",
      "are you \n",
      "shake it \n",
      "\n",
      "\n",
      "\n",
      "./checkpoints/textcnn/best_validation\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/textcnn/best_validation\n",
      "restored ./checkpoints/textcnn/best_validation\n",
      "\n",
      "\n",
      "\n",
      "Testing...\n",
      "\n",
      "\n",
      "Our classification model predict it to be: \n",
      "rock\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "run(\"rock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Rap lyric and verify\n",
    "We now use the 'rap' LSTM model to generate a rap lyric. And verify the result with our classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating...\n",
      "./checkpoints/rap/lyric-99\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/rap/lyric-99\n",
      "restored ./checkpoints/rap/lyric-99\n",
      "\n",
      "\n",
      "\n",
      "The generated lyrics: \n",
      "\n",
      "lil kim verse two \n",
      "uh , kill that shit , motherfucker \n",
      "\n",
      "and it not a dream \n",
      "hip - hop rule ? done the crew is \n",
      "youre my ego \n",
      "hoe grab to the \n",
      "la \n",
      "40 \n",
      "my hand in the titanic \n",
      "like crack in the spot thats whoever to make em tour \n",
      "the weapon been twisted ricky lucky hitler \n",
      "and all the time to try , we could tell him how he sits : like a cock \n",
      "\n",
      "( wyclef ) city \n",
      "can i get the clientele by the \n",
      "but now we better straw and continue deep , liftin the shore high \n",
      "if i do and like the jam ive usually alive \n",
      "what my life is than so military , who the fuck at the contract \n",
      "aint me drinking faster than you aroma of fluid \n",
      "i can feel my music a favor , dont fuck with what we mad \n",
      "and the mission of the improvement and these nigga is track or be playing g to bungalow backwards , nigga \n",
      "\n",
      "ru - eee - oh - dammit , ay , nigga , heh \n",
      "off the car , pull the people to the sign that there \n",
      "wire coat weed , ninja crowd screaming and the whole world more \n",
      "now i rap good , pale water , like this \n",
      "last intuition , i got plenty \n",
      "killing nigga with wesley getting money up \n",
      "theyll expose menace to the game , only you know just aint right \n",
      "fuck that , nigga that corner shit , a man break crack \n",
      "try cause they aint never meant to be another nigga \n",
      "brooklyn nigga we made the power of my story \n",
      "once theyll go out , nigga , put a great phenomenal speak \n",
      "you a hobby this that bitch , i got that top of the god \n",
      "so they catch you with something to be \n",
      "talk about to ride with the white spot \n",
      "take my crown , turned at the time ( we we made news \n",
      "we wa up out of they chain ) \n",
      "all we all tied down \n",
      "og fucking casa bringing the sweeper feature soundin feeling like pelican free \n",
      "hundred year in southside top like a new weight yeah \n",
      "man , find out when this isnt \n",
      "they saying for this \n",
      "\n",
      "cause the beef is smokin checker \n",
      "these nigga who comprehend , \n",
      "anybody know what they , they rap , nigga \n",
      "might catch it all to the good game get in the freezer \n",
      "\n",
      "yeah , all big nigga just man \n",
      "like cop like , more car wide \n",
      "we dont trust the real shit \n",
      "this that mean shit he wouldnt be some more \n",
      "i remember , callin them g - bounce like u \n",
      "hella busy nigga ? i dont talk shit to prove \n",
      "put the heat here and choose \n",
      "im tired of the same weed for that crip ? \n",
      "let your side wont send a roof in \n",
      "from being , motherfucker thought ? \n",
      "face to jones you got ta have to hate \n",
      "respect today just piss on your shoulder \n",
      "and if they just feel that i built a motherfucker stop \n",
      "if you hit me with this shit , i aint real enough \n",
      "to move this , never full of crack , handcuff change \n",
      "but i need you cause we could get to slump properly \n",
      "if youre doin what im supposed to get to go alone \n",
      "they stood like ice baby , see preme on lock \n",
      "celebrating that city , when the night is still free \n",
      "all the price is : the target \n",
      "your life been fame and hov is for you \n",
      "some sleep \n",
      "a young man , nigga , clap at sky bitch \n",
      "\n",
      "\n",
      "\n",
      "./checkpoints/textcnn/best_validation\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/textcnn/best_validation\n",
      "restored ./checkpoints/textcnn/best_validation\n",
      "\n",
      "\n",
      "\n",
      "Testing...\n",
      "\n",
      "\n",
      "Our classification model predict it to be: \n",
      "rap\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "run(\"rap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
